{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7438395,"sourceType":"datasetVersion","datasetId":4324835}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/killianmcguinness/data-visualisation-call-center-kpis?scriptVersionId=164988087\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# KPI Visualization for Contact Centers\n\n## Objective:\nLeverage a common call center dataset to craft visually engaging visualizations that shed light on the operational efficiency and effectiveness of the contact center. This project will delve into fundamental metrics and concepts pivotal in call center management. Before jumping in we deal with some common KPIs I want to define. \n\n- **CAR (Call Acceptance/Abandon Rate):**\n  - Definition: Volume of calls answered before the caller hangs up.\n\n- **SLA (Service Level Agreement):**\n  - Definition: Volume of calls answered within the agreed timeframe.\n\n- **Concurrency:**\n  - Definition: Amount of calls occurring simultaneously.\n\n- **Capacity:**\n  - Definition: The volume of calls that the actual number of agents can handle.\n\n- **Handle Times:**\n  - Definition: Average duration to manage a call type.\n\n\n## About The Data:\nThis dataset comprises typical call center data sourced from the City of Cincinnati, Ohio's Citizen's Information Centre. It specifically focuses on incoming calls to the contact center, representing information commonly exported from a Customer Relationship Management (CRM) or Soft-phone service.\n\n**Period:**\nOctober 1st 2022 tth December 31st 2022 (Q4 2022). \n\nThis project aims to present a visually engaging overview of call center performance metrics, providing valuable insights into the efficiency and effectiveness of the contact center operations.\n","metadata":{}},{"cell_type":"markdown","source":"| Step | Objective |\n|------|-----------|\n| 1    | Read in CSV data & make it easier to work with. |\n| 2    | Calculate CAR (Call Acceptance Rate) and SLA (Service Level Agreement). |\n| 3    | Visualise CAR & SLA performance using Dataframes. |\n| 4    | Analyze Call Concurrency and Agent Capacity. |\n| 5    | Building HeatMaps for better visualization. |\n| 6    | Refine data for Call Arrival Patterns & Average Handle Times. |\n| 7    | Forecast 12 Months Calls volumes using Time Series Model. |","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Data Preparation üõ†Ô∏è\n### Objective: Read in CSV data & make it easier to work with. <br>\n\n**1. Import Key Libraries:**\n- Import necessary libraries for data manipulation and visualization.\n\n**2. Data Loading & Cleaning:**\n- Read a call center dataset from a CSV file, focusing on relevant columns. Rename columns for clarity and add date-related features. üìÖ","metadata":{}},{"cell_type":"code","source":"# Importing python libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib as mpl\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.ticker import FuncFormatter\nfrom matplotlib import colormaps\nimport datetime\n\n# Choose a color Palette & Style sheet\ncolor_pal = sns.color_palette(\"tab20c\")\ncolor_brand = ['#ABE3C4', '#E7F7EE', '#5F8778', '#85B59E', '#E3ABCA', '#FFCCCC', '#F18C72', '#6BAED6', '#9ECAE1', '#D9D9D9']\n# Use the 'fivethirtyeight' style for Matplotlib\nplt.style.use('fivethirtyeight')\n# Define font styles for titles and sub-titles\ntitles_dict = {'fontsize': 28,\n 'fontweight': 25,\n 'color':   color_brand[2]}\n\nsub_title_dict = {'fontsize': 20,\n 'fontweight': 18,\n 'color':   color_brand[2]}\n\nfig_text_dict = {\n    'color':   color_brand[2], \n}\n\ntextprops={'color': color_brand[2], 'fontsize':8}","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:36:55.384894Z","iopub.execute_input":"2024-02-26T08:36:55.385335Z","iopub.status.idle":"2024-02-26T08:36:59.477976Z","shell.execute_reply.started":"2024-02-26T08:36:55.385299Z","shell.execute_reply":"2024-02-26T08:36:59.476618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the dataset and parse dates during reading\ndf = pd.read_csv('/kaggle/input/call-center-detailed/Citizen_Service_Request__CSR__Call_Center_Calls_20240120 (1).csv', parse_dates=['CONNCLEARDT', 'QUEUEENDDT', 'WRAPENDDT', 'QUEUESTARTDT'])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-02-26T08:36:59.480751Z","iopub.execute_input":"2024-02-26T08:36:59.4813Z","iopub.status.idle":"2024-02-26T08:37:11.130239Z","shell.execute_reply.started":"2024-02-26T08:36:59.481265Z","shell.execute_reply":"2024-02-26T08:37:11.128412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select only important columns\ncolumns = df.columns\ncolumns_important = [columns[3], columns[0], columns[6], columns[8], columns[13], columns[14], columns[18], columns[19], columns[20], columns[21], columns[22], columns[25]]\ndf_only_important = df[columns_important]\n\n# Rename columns, to make it easier to read\ndf_only_important.rename(columns={columns_important[0]:'detail_reason', columns_important[1]:'agent_id', columns_important[2]:'talk_time_ended', columns_important[3]: 'talk_time_started', columns_important[4]: 'wrap_ended', columns_important[5]:'call_started',columns_important[6]:'call_waiting', columns_important[7]:'talk_time', columns_important[8]:'wrap_time', columns_important[9]:'service_level_reached',columns_important[10]:'abandoned',columns_important[11]:'answered'}, inplace=True )\n\n# Add a few Data related features to make analysis easier\ndf_only_important['day_name'] = df_only_important['call_started'].dt.day_name()\ndf_only_important['day_number'] = df_only_important['call_started'].dt.weekday\ndf_only_important['week_number'] = df_only_important['call_started'].dt.isocalendar().week\ndf_only_important['month'] = df_only_important['call_started'].dt.month\ndf_only_important['date'] = df_only_important['call_started'].dt.date\ndf_only_important['hour'] = df_only_important['call_started'].dt.hour\ndf_only_important['time'] = df_only_important['call_started'].dt.time\ndf_only_important['rounded_time'] = pd.to_datetime(df_only_important['call_started']).dt.round('15min').dt.time","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-02-26T08:37:11.132365Z","iopub.execute_input":"2024-02-26T08:37:11.133546Z","iopub.status.idle":"2024-02-26T08:37:11.252919Z","shell.execute_reply.started":"2024-02-26T08:37:11.133492Z","shell.execute_reply":"2024-02-26T08:37:11.25173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Calculate CAR (Call Acceptance Rate) and SLA (Service Level Agreement).\n## Objective: Build sub Dataframes to make visualising CAR & SLA Easier <br>\n\n**1. Service Level Agreement (SLA):**\n- Calculate call abandonment rate (CAR) and SLA totals.\n- Group data by day, hour, and month to analyze SLA performance.\n- Compute the percentage of 'Yes' ('Y') and 'No' ('N') for SLA. üìä\n\n**2. Analyze Call Acceptance Rate (CAR):**\n- Group data by day, hour, and month to analyze CAR.\n- Calculate the percentage of 'Yes' and 'No' for CAR. üìà\n\n**3. SLA Target Calculation:**\n- Group data by date, week, and month to analyze SLA targets. üéØ\n","metadata":{}},{"cell_type":"code","source":"# Count the total number of 'answered' values (Y/N) in the dataset\ncar_totals = df_only_important['answered'].value_counts()\n# Count the total number of 'service_level_reached' values (Y/N) in the dataset\nsla_totals = df_only_important['service_level_reached'].value_counts()\n\n# Calculate SLA performance by day of the week\nsla_day_week = df_only_important.groupby(df_only_important['day_number'])['service_level_reached'].value_counts().unstack(fill_value=0)\nsla_day_week['%_Y'] = round((sla_day_week['Y'] / (sla_day_week['Y'] + sla_day_week['N'])) * 100, 2)\nsla_day_week['%_N'] = round((sla_day_week['N'] / (sla_day_week['Y'] + sla_day_week['N'])) * 100, 2)\n\n# Calculate SLA performance by hour of the day\nsla_hour = df_only_important.groupby(df_only_important['hour'])['service_level_reached'].value_counts().unstack(fill_value=0)\nsla_hour['%_Y'] = round((sla_hour['Y'] / (sla_hour['Y'] + sla_hour['N'])) * 100, 2)\nsla_hour['%_N'] = round((sla_hour['N'] / (sla_hour['Y'] + sla_hour['N'])) * 100, 2)\n\n# Calculate SLA performance by month\nsla_month = df_only_important.groupby(df_only_important['month'])['service_level_reached'].value_counts().unstack(fill_value=0)\nsla_month['%_Y'] = round((sla_month['Y'] / (sla_month['Y'] + sla_month['N'])) * 100, 2)\nsla_month['%_N'] = round((sla_month['N'] / (sla_month['Y'] + sla_month['N'])) * 100, 2)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:11.25454Z","iopub.execute_input":"2024-02-26T08:37:11.255732Z","iopub.status.idle":"2024-02-26T08:37:11.31506Z","shell.execute_reply.started":"2024-02-26T08:37:11.255688Z","shell.execute_reply":"2024-02-26T08:37:11.314141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate CAR by day of the week\ncar_day_week = df_only_important.groupby(df_only_important['day_number'])['abandoned'].value_counts().unstack(fill_value=0)\ncar_day_week['totals'] = (car_day_week['Y'] + car_day_week['N'])\ncar_day_week['%_N'] = round((car_day_week['N'] / car_day_week['totals']) * 100, 2)\ncar_day_week['%_Y'] = round((car_day_week['Y'] / car_day_week['totals']) * 100, 2)\n\n# Calculate CAR by hour of the day\ncar_hour = df_only_important.groupby(df_only_important['hour'])['abandoned'].value_counts().unstack(fill_value=0)\ncar_hour['%_N'] = round((car_hour['N'] / (car_hour['Y'] + car_hour['N'])) * 100, 2)\ncar_hour['%_Y'] = round((car_hour['Y'] / (car_hour['Y'] + car_hour['N'])) * 100, 2)\n\n# Calculate CAR by month\ncar_month = df_only_important.groupby(df_only_important['month'])['abandoned'].value_counts().unstack(fill_value=0)\ncar_month['%_N'] = round((car_month['N'] / (car_month['Y'] + car_month['N'])) * 100, 2)\ncar_month['%_Y'] = round((car_month['Y'] / (car_month['Y'] + car_month['N'])) * 100, 2)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:11.31846Z","iopub.execute_input":"2024-02-26T08:37:11.319647Z","iopub.status.idle":"2024-02-26T08:37:11.358749Z","shell.execute_reply.started":"2024-02-26T08:37:11.3196Z","shell.execute_reply":"2024-02-26T08:37:11.357474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate SLA target by date, week, and month\ndaily_vols_car_sla = df_only_important.groupby(df_only_important['date'])['service_level_reached'].value_counts().unstack(fill_value=0)\ndaily_vols_car_sla['sla_target'] = (daily_vols_car_sla['Y'] + daily_vols_car_sla['N'])*0.8\n\nweekly_vols_car_sla = df_only_important.groupby(df_only_important['week_number'])['service_level_reached'].value_counts().unstack(fill_value=0)\nweekly_vols_car_sla['sla_target'] = (weekly_vols_car_sla['Y'] + weekly_vols_car_sla['N'])*0.8\n\nmonthly_vols_car_sla = df_only_important.groupby(df_only_important['month'])['service_level_reached'].value_counts().unstack(fill_value=0)\nmonthly_vols_car_sla['sla_target'] = (monthly_vols_car_sla['Y'] + monthly_vols_car_sla['N'])*0.8","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:11.360388Z","iopub.execute_input":"2024-02-26T08:37:11.360876Z","iopub.status.idle":"2024-02-26T08:37:11.398831Z","shell.execute_reply.started":"2024-02-26T08:37:11.360836Z","shell.execute_reply":"2024-02-26T08:37:11.397595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Visualise CAR & SLA.\n## Objective: Using the Dataframes we built, Visualise CAR & SLA performance <br>\n\n**1. Figure Setup:**\n- Create a figure with tight layout, emphasizing the main title. üñºÔ∏è\n- Define a grid layout with 6 rows and 4 columns for subplots.\n\n**2. Subplot Formatting:**\n- Define dictionaries for title and subtitle font settings. üìù\n- Define text properties for additional formatting.\n\n**3. Plot:**\n- Group data by date, week, and month to analyze SLA targets. üìä","metadata":{}},{"cell_type":"code","source":"# Create a figure and plot our CAR and SLA Views!\n\nfig = plt.figure(tight_layout=True, figsize=(20, 40))\ngs = gridspec.GridSpec(6, 4)\n\n# SLA Totals\nax1 = fig.add_subplot(gs[0, 0])\ncustom_labels = [f\"In SLA: {sla_totals[0]}\", f\"Out SLA: {sla_totals[1]}\"]\nax1.pie(sla_totals, labels=custom_labels, autopct='%1.1f%%', startangle=140, colors=[color_brand[0], color_brand[5]], textprops=fig_text_dict)\nax1.set_title('SLA: Total Q1', fontdict=titles_dict)\n\n# SLA Monthly\nax2 = fig.add_subplot(gs[1,0])\nax2.bar(sla_month.index, sla_month['%_Y'], color=color_brand[0], label='In SLA')\nax2.bar(sla_month.index, sla_month['%_N'], bottom=sla_month['%_Y'], color=color_brand[5], label='Not in SLA')\nax2.set_title('SLA: Monthly', fontdict=titles_dict)\nax2.set_xlabel('Month')\nax2.set_ylabel('SLA%')\nax2.set_ylim(40, 100)\nax2.set_xticks(sla_month.index)\nax2.set_xticklabels(['Jan', 'Feb', 'Mar'])\nax2.axhline(y=80, color=color_brand[4], linestyle='--', label='Target: 80%')\nax2.legend()\n\n# SLA Daily\nax3 = fig.add_subplot(gs[0, 1:])\nax3.bar(sla_day_week.index, sla_day_week['%_Y'], color=color_brand[0], label='In SLA')\nax3.bar(sla_day_week.index, sla_day_week['%_N'], bottom=sla_day_week['%_Y'], color=color_brand[5], label='Not in SLA')\nax3.set_title('SLA: Avg Day Week', fontdict=titles_dict)\nax3.set_xlabel('Day')\nax3.set_ylabel('SLA%')\nax3.set_ylim(40, 100)\nax3.set_xticks(sla_day_week.index)\nax3.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri'])\nax3.axhline(y=80, color=color_brand[4], linestyle='--', label='Target: 80%')\nax3.legend()\n\n# SLA Hourly\nax4 = fig.add_subplot(gs[1, 1:])\nax4.barh(sla_hour.index, sla_hour['%_Y'], color=color_brand[0], label='In SLA')\nax4.barh(sla_hour.index, sla_hour['%_N'], left=sla_hour['%_Y'], color=color_brand[5], label='Not in SLA')\nax4.set_title('SLA: Avg Hour Interval', fontdict=titles_dict)\nax4.set_xlabel('SLA%')\nax4.set_ylabel('Hour Interval')\nax4.set_yticks(sla_hour.index)\nax4.axvline(x=80, color=color_brand[4], linestyle='--', label='Target: 80%')\nax4.legend()\n\n# CAR Totals\nax5 = fig.add_subplot(gs[2,0])\ncustom_labels = [f\"Accepted: {car_totals[0]}\", f\"Dropped: {car_totals[1]}\"]\nax5.pie(car_totals, labels=custom_labels, autopct='%1.1f%%', startangle=140, colors=[color_brand[0], color_brand[5]], textprops=fig_text_dict)\nax5.set_title('CAR: Total Q1', fontdict=titles_dict)\n\n# CAR Monthly\nax6 = fig.add_subplot(gs[3,0])\nax6.bar(car_month.index, car_month['%_N'], color=color_brand[0], label='%Answer')\nax6.bar(car_month.index, car_month['%_Y'], bottom=car_month['%_N'], color=color_brand[5], label='%Drop')\nax6.set_title('CAR: Monthly', fontdict=titles_dict)\nax6.set_xlabel('Month')\nax6.set_ylabel('CAR%')\nax6.set_ylim(40, 100)\nax6.set_xticks(car_month.index)\nax6.set_xticklabels(['Jan', 'Feb', 'Mar'])\nax6.axhline(y=80, color=color_brand[4], linestyle='--', label='Target: 80%')\nax6.legend()\n\n# CAR Daily\nax7 = fig.add_subplot(gs[2, 1:])\nax7.bar(car_day_week.index, car_day_week['%_N'], color=color_brand[0], label='%Answer')\nax7.bar(car_day_week.index, car_day_week['%_Y'], bottom=car_day_week['%_N'], color=color_brand[5], label='%Drop')\nax7.set_title('CAR: Avg Daily', fontdict=titles_dict)\nax7.set_xlabel('Day of Week')\nax7.set_ylabel('CAR%')\nax7.set_ylim(20, 100)\nax7.set_xticks(car_day_week.index)\nax7.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri'])\nax7.axhline(y=80, color=color_brand[4], linestyle='--', label='Target: 80%')\nax7.legend()\n\n# CAR Hourly\nax8 = fig.add_subplot(gs[3, 1:])\nax8.barh(car_hour.index, car_hour['%_N'], color=color_brand[0], label='%Answer')\nax8.barh(car_hour.index, car_hour['%_Y'], left=car_hour['%_N'], color=color_brand[5], label='%Drop')\nax8.set_title('CAR: Avg Hourly Interval', fontdict=titles_dict)\nax8.set_xlabel('CAR%')\nax8.set_ylabel('Hour Interval')\nax8.set_yticks(car_hour.index)\nax8.axvline(x=80, color=color_brand[4], linestyle='--', label='Target: 80%')\nax8.legend()\n    \n# CAR / SLA Daily\nax9 = fig.add_subplot(gs[4, 0:])\nax9.bar(daily_vols_car_sla.index, daily_vols_car_sla['N'], color=color_brand[0], label='In SLA')\nax9.bar(daily_vols_car_sla.index, daily_vols_car_sla['Y'], bottom=daily_vols_car_sla['N'], color=color_brand[5], label='Not in SLA')\nax9.plot(daily_vols_car_sla['sla_target'], color=color_brand[4], label='SLA Target', linestyle='--')\nax9.set_title('SLA Daily', fontdict=titles_dict)\nax9.set_xlabel('Day')\nax9.set_ylabel('CAR & SLA')\nax9.legend()\n\n# CAR / SLA Weekly\nax10 = fig.add_subplot(gs[5, 0:2])\nax10.bar(weekly_vols_car_sla.index, weekly_vols_car_sla['N'], color=color_brand[0], label='In SLA')\nax10.bar(weekly_vols_car_sla.index, weekly_vols_car_sla['Y'], bottom=weekly_vols_car_sla['N'], color=color_brand[5], label='Not in SLA')\nax10.plot(weekly_vols_car_sla['sla_target'], color=color_brand[4], label='SLA Target', linestyle='--')\nax10.set_title('SLA Weekly', fontdict=titles_dict)\nax10.set_xlabel('Week Number')\nax10.set_ylabel('CAR & SLA')\nax10.legend()\n\n# CAR / SLA Monthly\nax11 = fig.add_subplot(gs[5, 2:])\nax11.bar(monthly_vols_car_sla.index, monthly_vols_car_sla['N'], color=color_brand[0], label='In SLA')\nax11.bar(monthly_vols_car_sla.index, monthly_vols_car_sla['Y'], bottom=monthly_vols_car_sla['N'], color=color_brand[5], label='Not in SLA')\nax11.plot(monthly_vols_car_sla['sla_target'], color=color_brand[4], label='SLA Target', linestyle='--')\nax11.set_title('SLA Monthly', fontdict=titles_dict)\nax11.set_xlabel('Month')\nax11.set_ylabel('CAR & SLA')\nax11.set_xticklabels(['Jan', 'Feb', 'Mar'])\nax11.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:11.400966Z","iopub.execute_input":"2024-02-26T08:37:11.401354Z","iopub.status.idle":"2024-02-26T08:37:15.970329Z","shell.execute_reply.started":"2024-02-26T08:37:11.401324Z","shell.execute_reply":"2024-02-26T08:37:15.96908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Analyze Call Concurrency and Agent Capacity.\n### Objective: Build Dataframes to measure call concurrency (How many calls are happening at once) and Agent Capacity (How many Agents are available). <br>\n\n**1. Data Preparation:**\n- Filter out rows with missing values in the 'talk_time_started' column. üßπ\n- Select relevant columns for further analysis.\n- Create 'start' and 'stop' columns to represent call and talk time intervals.\n- Concatenate call and talk time series along with the status series.\n\n**2. Sorting and Counter Calculation:**\n- Sort the DataFrame by call time.\n- Calculate the cumulative counter based on 'start' and 'stop' status.\n\n**3. Hourly Concurrency Analysis:**\n- Group by hour and calculate the 75th percentile of the counter. üìà\n- Extract the week number from call time and group by week and hour.\n- Pivot the DataFrame to create a heatmap-ready structure for weekly hourly concurrency.\n\n**4. Agent Capacity Analysis:**\n- Group by hour and calculate the number of unique agents. üïµÔ∏è‚Äç‚ôÇÔ∏è\n- Group by week and hour, calculate the number of unique agents for weekly analysis.\n\n**5. Delta Calculation:**\n- Concatenate hourly agent count, concurrent calls, and calculate the delta (needed_agents - actual_agents).\n- Concatenate weekly hourly agent count, concurrent calls, and calculate the delta for weekly analysis.\n- Pivot the DataFrame to create a heatmap-ready structure for delta analysis.\n\n**6. Conclusion:**\n- Perform a comprehensive analysis of call concurrency and agent capacity.\n- Visualize hourly concurrency, agent capacity, and the delta between needed and actual agents on both daily and weekly scales.\n- Utilize appropriate titles, labels, legends, and color coding for clarity. üé®","metadata":{}},{"cell_type":"code","source":"# Filter out rows with missing values in 'talk_time_started'\ndf_rough = df_only_important.dropna(subset=['talk_time_started'])\n\n# Select only the relevant columns\ndf_rough = df_rough[['call_started', 'talk_time_started']]\n\n# Create 'start' and 'stop' columns\ndf_rough['start'] = 'start'\ndf_rough['stop'] = 'stop'\n\n# Concatenate call and talk time series along with status series\ncalls_series = pd.concat([df_rough['call_started'], df_rough['talk_time_started']], ignore_index=True)\nstatus_series = pd.concat([df_rough['start'], df_rough['stop']], ignore_index=True)\ncalls_concurrent = pd.concat([calls_series, status_series], axis=1)\ncalls_concurrent.columns = ['call_time', 'status']\n\n# Sort the DataFrame by call time\ncalls_concurrent = calls_concurrent.sort_values(by='call_time')\n\n# Calculate the cumulative counter based on 'start' and 'stop' status\ncalls_concurrent['counter'] = np.where(calls_concurrent['status'].eq('start'), 1, -1).cumsum()\n\n# Group by hour and calculate the 75th percentile of the counter\ncalls_concurrent_hour = calls_concurrent.groupby(calls_concurrent['call_time'].dt.hour)['counter'].quantile(0.75).reset_index()\n\n# Extract week number from call time and group by week and hour\ncalls_concurrent['week_number'] = calls_concurrent['call_time'].dt.isocalendar().week\ncalls_concurrent_week_hour = calls_concurrent.groupby(['week_number', calls_concurrent['call_time'].dt.hour])['counter'].quantile(0.75).reset_index()\ncalls_concurrent_week_hour = calls_concurrent_week_hour.reset_index(drop=True)\n\n# Pivot the DataFrame to create a heatmap-ready structure\nheatmap_data = calls_concurrent_week_hour.pivot(index='call_time', columns='week_number', values='counter')\n\n# Group by hour and calculate the number of unique agents\nhourly_agents = df_only_important.groupby(df_only_important['talk_time_started'].dt.hour)['agent_id'].nunique().reset_index()\nhourly_agents.columns = ['hour', 'num_agents']\n\n# Group by week and hour, calculate the number of unique agents\nweekly_hourly_agents = df_only_important.groupby([df_only_important['talk_time_started'].dt.isocalendar().week, df_only_important['talk_time_started'].dt.hour])['agent_id'].nunique().reset_index()\nweekly_hourly_agents.columns = ['week', 'hour', 'num_agents']\n","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:15.971978Z","iopub.execute_input":"2024-02-26T08:37:15.972428Z","iopub.status.idle":"2024-02-26T08:37:16.062982Z","shell.execute_reply.started":"2024-02-26T08:37:15.972387Z","shell.execute_reply":"2024-02-26T08:37:16.06167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Analyzing hourly agents and their delta against needed agents\n# Hourly Agents\nhourly_agents = df_only_important.groupby(df_only_important['talk_time_started'].dt.hour)['agent_id'].nunique().reset_index()\nhourly_agents.columns = ['hour', 'num_agents']\n\n# Weekly Hourly Agents\nweekly_hourly_agents = df_only_important.groupby([df_only_important['talk_time_started'].dt.isocalendar().week, df_only_important['talk_time_started'].dt.hour])['agent_id'].nunique().reset_index()\nweekly_hourly_agents.columns = ['week', 'hour', 'num_agents']\n\n# Concatenate hourly agent data with concurrent call data\nagent_delta_hour = pd.concat([hourly_agents['hour'], hourly_agents['num_agents'], calls_concurrent_hour['counter']], axis=1)\nagent_delta_hour.columns = ['hour', 'actual_agents', 'needed_agents']\nagent_delta_hour['delta'] = agent_delta_hour['needed_agents'] - agent_delta_hour['actual_agents']\n\n# Concatenate weekly hourly agent data with weekly concurrent call data\nagent_delta_hour_weekly = pd.concat([weekly_hourly_agents['week'], weekly_hourly_agents['hour'], weekly_hourly_agents['num_agents'], calls_concurrent_week_hour['counter']], axis=1)\nagent_delta_hour_weekly.columns = ['week_number', 'hour', 'actual_agents', 'needed_agents']\nagent_delta_hour_weekly['delta'] = agent_delta_hour_weekly['needed_agents'] - agent_delta_hour_weekly['actual_agents']\n\n# Prepare data for heatmap\nheatmap_delta_data = agent_delta_hour_weekly.pivot(index='hour', columns='week_number', values='delta')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:16.064867Z","iopub.execute_input":"2024-02-26T08:37:16.065615Z","iopub.status.idle":"2024-02-26T08:37:16.104816Z","shell.execute_reply.started":"2024-02-26T08:37:16.06555Z","shell.execute_reply":"2024-02-26T08:37:16.103483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Building HeatMaps\n### Objective: Build a function to make creating & decorating Heatmaps easier. <br>\n\n**1. Function Purpose:**\n   - Create a heatmap from a 2D numpy array with row and column labels. üó∫Ô∏è\n\n**2. Parameters:**\n   - `data`: A 2D numpy array of shape (M, N).\n   - `row_labels`: A list or array of length M with labels for the rows.\n   - `col_labels`: A list or array of length N with labels for the columns.\n   - `ax`: A `matplotlib.axes.Axes` instance to plot the heatmap (optional).\n   - `cbar_kw`: A dictionary with arguments to `matplotlib.Figure.colorbar` (optional).\n   - `cbarlabel`: The label for the colorbar (optional).\n   - `**kwargs`: Additional arguments forwarded to `imshow`.\n\n**3. Implementation:**\n   - Plot the heatmap using `ax.imshow(data, **kwargs)`.\n   - Create a colorbar using `matplotlib.Figure.colorbar`.\n   - Set ticks and labels for rows and columns.\n   - Customize tick labels' appearance and grid lines. üåà\n\n### Annotation Function (`annotate_heatmap`)\n\n**1. Function Purpose:**\n   - Annotate values on a heatmap.\n\n**2. Parameters:**\n   - `im`: The AxesImage to be labeled.\n   - `data`: Data used to annotate (optional).\n   - `valfmt`: The format of the annotations (optional).\n   - `textcolors`: A pair of colors for values above and below a threshold (optional).\n   - `threshold`: Value in data units for color separation (optional).\n   - `**textkw`: Additional arguments forwarded to each call to `text` for label creation.\n\n**3. Implementation:**\n   - Normalize the threshold.\n   - Loop over the data and create `Text` for each \"pixel.\"\n   - Change text color based on the data value and threshold.\n   - Customize text alignment and color. üé®","metadata":{}},{"cell_type":"code","source":"def heatmap(data, row_labels, col_labels, ax=None,\n            cbar_kw=None, cbarlabel=\"\", **kwargs):\n    \"\"\"\n    Create a heatmap from a numpy array and two lists of labels.\n\n    Parameters\n    ----------\n    data\n        A 2D numpy array of shape (M, N).\n    row_labels\n        A list or array of length M with the labels for the rows.\n    col_labels\n        A list or array of length N with the labels for the columns.\n    ax\n        A `matplotlib.axes.Axes` instance to which the heatmap is plotted.  If\n        not provided, use current axes or create a new one.  Optional.\n    cbar_kw\n        A dictionary with arguments to `matplotlib.Figure.colorbar`.  Optional.\n    cbarlabel\n        The label for the colorbar.  Optional.\n    **kwargs\n        All other arguments are forwarded to `imshow`.\n    \"\"\"\n\n    if ax is None:\n        ax = plt.gca()\n\n    if cbar_kw is None:\n        cbar_kw = {}\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n\n    # Show all ticks and label them with the respective list entries.\n    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False,\n                   labeltop=True, labelbottom=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n\n    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    return im, cbar\n\n\ndef annotate_heatmap(im, data=None, valfmt=\"{x:.0f}\",\n                     textcolors=(\"black\", \"white\"),\n                     threshold=None, **textkw):\n    \"\"\"\n    A function to annotate a heatmap.\n\n    Parameters\n    ----------\n    im\n        The AxesImage to be labeled.\n    data\n        Data used to annotate.  If None, the image's data is used.  Optional.\n    valfmt\n        The format of the annotations inside the heatmap.  This should either\n        use the string format method, e.g. \"$ {x:.2f}\", or be a\n        `matplotlib.ticker.Formatter`.  Optional.\n    textcolors\n        A pair of colors.  The first is used for values below a threshold,\n        the second for those above.  Optional.\n    threshold\n        Value in data units according to which the colors from textcolors are\n        applied.  If None (the default) uses the middle of the colormap as\n        separation.  Optional.\n    **kwargs\n        All other arguments are forwarded to each call to `text` used to create\n        the text labels.\n    \"\"\"\n\n    if not isinstance(data, (list, np.ndarray)):\n        data = im.get_array()\n\n    # Normalize the threshold to the images color range.\n    if threshold is not None:\n        threshold = im.norm(threshold)\n    else:\n        threshold = im.norm(data.max())/2.\n\n    # Set default alignment to center, but allow it to be\n    # overwritten by textkw.\n    kw = dict(horizontalalignment=\"center\",\n              verticalalignment=\"center\")\n    kw.update(textkw)\n\n    # Get the formatter in case a string is supplied\n    if isinstance(valfmt, str):\n        valfmt = mpl.ticker.StrMethodFormatter(valfmt)\n\n    # Loop over the data and create a `Text` for each \"pixel\".\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)\n            texts.append(text)\n\n    return texts","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:16.106896Z","iopub.execute_input":"2024-02-26T08:37:16.107555Z","iopub.status.idle":"2024-02-26T08:37:16.129651Z","shell.execute_reply.started":"2024-02-26T08:37:16.107511Z","shell.execute_reply":"2024-02-26T08:37:16.1281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Call Arrival Patterns & Average Handle Times.\n### Objective: Refine our Data to make visualising Arrival Patterns & Handle Times easier. <br>\n\n### Data Preparation for Call Patterns (`call_patterns`)\n\n**1. Clean and Map Time Intervals:**\n   - Extract 'rounded_time' series and drop missing values. üï∞Ô∏è\n   - Map unique intervals to a specified range.\n   - Convert the date series to a DataFrame (`call_arrivals_15mins`).\n   - Use `value_counts` to get counts for each interval category.\n\n**2. Format x-axis Tick Labels:**\n   - Convert mapped intervals to appropriate time format (hh:mm). ‚è∞\n   - Build a list of formatted x-axis tick labels (`x_ticks_arrivals`).\n\n### Handle Times Analysis (`df_handle_times`, `handle_time_reason`, `handle_time_agent`)\n\n**1. Extract Relevant Columns:**\n   - Select relevant columns ('detail_reason', 'agent_id', 'talk_time', 'wrap_time') from `df_only_important`.\n   - Calculate the total handle time by summing 'talk_time' and 'wrap_time'. üîÑ\n\n**2. Reason-wise Handle Time Analysis:**\n   - Group by 'detail_reason' and aggregate metrics like maximum agent ID, average talk time, average wrap time, and average total handle time.\n   - Convert 'detail_reason' to a categorical type.\n\n**3. Agent-wise Handle Time Analysis:**\n   - Group by 'agent_id' and aggregate similar metrics as in reason-wise analysis.\n   - Convert 'agent_id' to a categorical type. üïµÔ∏è‚Äç‚ôÇÔ∏è\n\n**Note:** Commented-out lines related to max/min handle times are present in the code but not used in the final output.","metadata":{}},{"cell_type":"code","source":"# Processing call arrival intervals\n\n# Extract and preprocess call arrival times\ncall_patterns = df_only_important['rounded_time']\ncall_patterns = call_patterns.dropna()\n\n# Create unique call intervals\nunique_call_intervals = sorted(call_patterns.unique().tolist())\n\n# Define time range and step for mapping intervals\nstart = 11.0\nend = 22.0\nstep = 0.25\n\n# Map intervals to a specified range\nmapped_ints_intervals = [i for i in np.arange(start, end + step, step)]\n\n# Convert the date series to a DataFrame\ncall_patterns = call_patterns.replace(dict(zip(unique_call_intervals, mapped_ints_intervals)))\n\n# Convert the date series to a DataFrame\ncall_arrivals_15mins = pd.DataFrame({'interval': call_patterns})\n\n# # Use value_counts to get counts for each category\ncall_arrivals_15mins = call_arrivals_15mins.groupby('interval').size().reset_index(name='mean_count')\n\n# # # Convert counts to a DataFrame with appropriate column names\ncall_arrivals_15mins = call_arrivals_15mins.reset_index()\ncall_arrivals_15mins = call_arrivals_15mins.sort_values(by='interval')\ncall_arrivals_15mins['avg_interval'] = call_arrivals_15mins['mean_count'] / 61","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:16.131053Z","iopub.execute_input":"2024-02-26T08:37:16.131419Z","iopub.status.idle":"2024-02-26T08:37:16.257912Z","shell.execute_reply.started":"2024-02-26T08:37:16.131389Z","shell.execute_reply":"2024-02-26T08:37:16.25644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"call_arrivals_15mins.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:16.259229Z","iopub.execute_input":"2024-02-26T08:37:16.260196Z","iopub.status.idle":"2024-02-26T08:37:16.281337Z","shell.execute_reply.started":"2024-02-26T08:37:16.260162Z","shell.execute_reply":"2024-02-26T08:37:16.279618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mapping and formatting intervals for x-axis ticks\n\n# Convert mapped intervals to strings\nmapped_ints_intervals = [str(i) for i in mapped_ints_intervals]\nx_ticks_arrivals = []\n# Iterate over mapped intervals and format for display\nfor item in mapped_ints_intervals:\n    item_split = item.split('.')\n    if item_split[1] == '0':\n        x_ticks_arrivals.append(f'{item_split[0]}:00')\n    elif item_split[1] == '25':\n        x_ticks_arrivals.append(f'{item_split[0]}:15')\n    elif item_split[1] == '5':\n        x_ticks_arrivals.append(f'{item_split[0]}:30')\n    elif item_split[1] == '75':\n        x_ticks_arrivals.append(f'{item_split[0]}:45')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:16.282715Z","iopub.execute_input":"2024-02-26T08:37:16.283088Z","iopub.status.idle":"2024-02-26T08:37:16.290797Z","shell.execute_reply.started":"2024-02-26T08:37:16.283048Z","shell.execute_reply":"2024-02-26T08:37:16.289865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract relevant columns for handle times analysis\ndf_handle_times = df_only_important[['detail_reason', 'agent_id', 'talk_time', 'wrap_time', 'service_level_reached']]\n\n# Calculate total handle time by summing talk_time and wrap_time\ndf_handle_times['total_handle'] = df_only_important['talk_time'] + df_only_important['wrap_time']\n\n# Group by 'detail_reason' and calculate aggregated statistics for handle times\nhandle_time_reason = df_handle_times.groupby('detail_reason').agg({'agent_id':'max', 'talk_time': 'mean', 'talk_time': 'mean', 'wrap_time': 'mean', 'total_handle': 'mean', 'service_level_reached' : 'count'}).reset_index()\n# max_handle_time_reason = df_handle_times.groupby('detail_reason').max().reset_index()\n# min_handle_time_reason = df_handle_times.groupby('detail_reason').min().reset_index()\n# handle_time_reason['max_total'] = max_handle_time_reason['total_handle']\n# handle_time_reason['min_total'] = min_handle_time_reason['total_handle']\n\n# Group by 'agent_id' and calculate aggregated statistics for handle times\nhandle_time_agent = df_handle_times.groupby('agent_id').agg({'detail_reason':'max', 'talk_time': 'mean', 'talk_time': 'mean', 'wrap_time': 'mean', 'total_handle': 'mean'}).reset_index()\n# max_handle_time_agent = df_handle_times.groupby('agent_id').max().reset_index()\n# min_handle_time_agent = df_handle_times.groupby('agent_id').min().reset_index()\n# handle_time_agent['max_total'] = handle_time_agent['total_handle']\n# handle_time_agent['min_total'] = handle_time_agent['total_handle']\n\n# Convert 'agent_id' and 'detail_reason' to categorical data type\nhandle_time_agent['agent_id'] = handle_time_agent['agent_id'].astype('category')\nhandle_time_reason['detail_reason'] = handle_time_reason['detail_reason'].astype('category')","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:16.294521Z","iopub.execute_input":"2024-02-26T08:37:16.295817Z","iopub.status.idle":"2024-02-26T08:37:16.332786Z","shell.execute_reply.started":"2024-02-26T08:37:16.295729Z","shell.execute_reply":"2024-02-26T08:37:16.330737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 7: Forecasting 12 Months Calls Volumes\n\n### Objective: Use Time Series Forecasting Model to forecast the Next 12 Months of call Volumes üìà <br>\n\n**1. Group Data for Time Series:**\n   - Group `df_only_important` by date and calculate the size to create `projection_data`.\n\n**2. Handle Outliers in the Time Series:**\n   - Calculate the 75th percentile and mean of the time series. üìä\n   - Cap values in `projection_data` above the 75th percentile by setting them to the 75th percentile.\n\n**3. Fill Weekends with Mean Values:**\n   - Create a date range from 'start_date' to 'end_date'.\n   - Identify weekend dates in the range.\n   - Create a DataFrame `add_on` with weekend dates and mean projection values.\n   - Concatenate `add_on` to `projection_data` to fill weekends with mean values. üóìÔ∏è\n\n**4. Format and Sort Projection Data:**\n   - Convert 'date' to datetime in `projection_data`.\n   - Sort `projection_data` by 'date' and reset the index.\n\n### Prophet Time Series Forecasting\n\n**1. Define Model and Fit:**\n   - Initialize a Prophet model with linear growth and seasonality prior scale.\n   - Fit the model using `projection_data`.\n\n**2. Generate Future Dates:**\n   - Create a DataFrame `future` with future dates using `make_future_dataframe`.\n\n**3. Generate Forecast:**\n   - Use the fitted model to generate a forecast for the future dates.\n\n**4. Extract Relevant Forecast Columns:**\n   - Extract relevant columns ('ds', 'yhat', 'yhat_lower', 'yhat_upper') from the forecast for further analysis (`forecast_only_important`).\n\n**Note:** The seasonality prior scale is set to 10.0, and other Prophet parameters can be adjusted based on the specific characteristics of the time series data. üï∞Ô∏è\n","metadata":{}},{"cell_type":"code","source":"from prophet import Prophet\nfrom prophet.diagnostics import performance_metrics\n\n# Group data by date and calculate the count of occurrences\nprojection_data = df_only_important.groupby(df_only_important['date']).size().reset_index(name='y')\n\n# Calculate the 75th percentile and mean of the 'y' values\nprojection_75th = projection_data['y'].quantile(0.75)\nprojection_mean = projection_data['y'].mean()\n\n# Set values in 'y' column above the 75th percentile to the 75th percentile\nprojection_data.loc[projection_data['y'] > projection_75th, 'y'] = projection_75th\n\n# Define start and end dates for the analysis\nstart_date = '2021-01-04'\nend_date = '2021-03-31'\n\n# Create a date range from start to end date with a frequency of one day\ndate_range = pd.date_range(start=start_date, end=end_date, freq='D')\n\n# Select weekend dates from the date range\nweekend_dates = date_range[date_range.dayofweek.isin([5, 6])]\n\n# Create a DataFrame with weekend dates and the mean value\nadd_on = pd.DataFrame({'date': weekend_dates, 'y': [projection_mean] * len(weekend_dates)})\n\n# Concatenate the original projection_data with the add_on DataFrame\nprojection_data = pd.concat([projection_data, add_on], ignore_index=True)\n\n# Convert 'date' column to datetime\nprojection_data['date'] = pd.to_datetime(projection_data['date'])  # Convert 'date' to datetime\n\n# Sort the DataFrame by 'date' and reset the index\nprojection_data.sort_values(by='date', inplace=True)\nprojection_data.reset_index(drop=True, inplace=True)\n\n# Rename columns to use Prophet's conventions\nprojection_data.columns = ['ds', 'y']\n\n# Create a Prophet model with linear growth and a specified seasonality prior scale\nm = Prophet(growth='linear', seasonality_prior_scale=10.0)\n# Fit the model using the prepared projection_data\nm.fit(projection_data)\n# Generate a DataFrame for future dates\nfuture = m.make_future_dataframe(periods=365)\n# Make predictions for future dates using the trained model\nforecast = m.predict(future)\n# Select relevant columns from the forecast DataFrame\nforecast_only_important = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-02-26T08:37:16.335021Z","iopub.execute_input":"2024-02-26T08:37:16.335829Z","iopub.status.idle":"2024-02-26T08:37:18.493482Z","shell.execute_reply.started":"2024-02-26T08:37:16.33578Z","shell.execute_reply":"2024-02-26T08:37:18.492341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Based on our Projections from Prophet figure out Monthly agent needs & rough Financial forecast\nimport math\n\n# Add a col to total_resons df calculating the proportion of Total each detail reason represents\ntotal_reasons_ = handle_time_reason['service_level_reached'].sum()\nhandle_time_reason['%share'] = handle_time_reason['service_level_reached'] / total_reasons_\n\n# Average weighted AHT\nhandle_time_reason['weighted_aht'] =  handle_time_reason['total_handle'] * handle_time_reason['%share']\ntotal_weighted_aht = handle_time_reason['weighted_aht'].sum()\n\nagents_projections_daily = forecast_only_important\nagents_projections_daily.head()\n\nagents_projections_daily['total_seconds_work'] = agents_projections_daily['yhat'] * total_weighted_aht\n\nsla_ = 0.85\nmonthly_loaded_cost = 4792.77\n# Assuming an 8 Hour working shit, how many agents needed to cover to work \nagents_projections_daily['agents_needed_bare'] = (agents_projections_daily['total_seconds_work'] / (3600 * sla_) / 8)\nagents_projections_daily['agents_needed_w_shrinkage'] = agents_projections_daily['agents_needed_bare']  * 1.2\nagents_projections_daily['agents_needed_w_shrinkage_rounded'] = agents_projections_daily['agents_needed_w_shrinkage'].apply(lambda x: math.ceil(x))\n\n# handle_time_reason = df_handle_times.groupby('detail_reason').agg({'agent_id':'max', 'talk_time': 'mean', 'talk_time': 'mean', 'wrap_time': 'mean', 'total_handle': 'mean', 'service_level_reached' : 'count'}).reset_index()\nagents_projections_monthly = agents_projections_daily.groupby(pd.Grouper(key='ds', freq='M')).agg({\"agents_needed_bare\" : \"max\", \"agents_needed_w_shrinkage\" : \"max\", \"agents_needed_w_shrinkage_rounded\" : \"max\", \"yhat\" : \"sum\"})\nagents_projections_monthly['cost_bsl'] = agents_projections_monthly['agents_needed_w_shrinkage_rounded'] * monthly_loaded_cost","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-02-26T08:37:18.494983Z","iopub.execute_input":"2024-02-26T08:37:18.495317Z","iopub.status.idle":"2024-02-26T08:37:18.522465Z","shell.execute_reply.started":"2024-02-26T08:37:18.495288Z","shell.execute_reply":"2024-02-26T08:37:18.521135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data preparation for Projections & Costs\nx = agents_projections_monthly.index.tolist()\ny = agents_projections_monthly['cost_bsl'].tolist()\nxlabels = [item.strftime('%b') for item in agents_projections_monthly.index.tolist()]\nunique_costs_ = agents_projections_monthly.drop_duplicates(subset='cost_bsl', keep='first')\nx_costs = unique_costs_.index.tolist()\ny_costs = unique_costs_['cost_bsl'].tolist()\nlabels_costs = [f\"${str(item)[:2]}k\" for item in y_costs]","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:18.52387Z","iopub.execute_input":"2024-02-26T08:37:18.524912Z","iopub.status.idle":"2024-02-26T08:37:18.534216Z","shell.execute_reply.started":"2024-02-26T08:37:18.524868Z","shell.execute_reply":"2024-02-26T08:37:18.532946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 8: Visualizing Heatmaps, Agent Handle Times, and Time Series Forecast\n\n### Objective: Use the Dataframes built to visualize Call Arrival Patterns, Handle Times & Forecast Model üìä <br>\n\n**1. Concurrent Calls per Hour:**\n   - Create a HeatMap for concurrent calls per hour across different weeks.\n   - Use a reversed RdYlGn color map to emphasize variations. üåà\n   - Include a color bar to show the intensity of concurrent calls.\n   - Annotate the HeatMap for additional context.\n\n**2. Delta: Actuals vs Needed Agents:**\n   - Generate a HeatMap illustrating the difference between actual and needed agents per hour.\n   - Utilize a reversed RdYlGn color map and annotate for clear visibility.\n   - Include a color bar for quick reference.\n\n**3. Call Arrivals Distribution:**\n   - Display a horizontal bar chart for call arrivals distribution over 15-minute intervals.\n   - Use custom x-tick labels for improved readability. üìä\n   - Add titles and labels to enhance interpretation.\n\n**4. Average Handle Times: Agent and Reason:**\n   - Present two horizontal bar charts for average talk time and wrap time for agents and reasons.\n   - Use distinct colors for talk time and wrap time, and include legends for clarity.\n\n**5. Time Series Forecasting:**\n   - Plot actual call volume against the forecast for the upcoming 12 months.\n   - Visualize the prediction, lower and upper bounds, and highlight a specific date for reference. üìà\n   - Provide labels and titles for a comprehensive understanding.\n\n**Note:** The color palette and style can be customized based on preferences and visual consistency. üé®\n","metadata":{}},{"cell_type":"code","source":"# Define constants for concurrent weeks, hours, and color map\nCONCURRENT_WEEKS = ['W 1', 'W 2', 'W 3', 'W 4', 'W 5', 'W 6', 'W 7', 'W 8', 'W 9', 'W 10', 'W 11', 'W 12', 'W 13']\nCONCURRENT_HOURS = ['11am', '12pm', '1pm', '2pm', '3pm', '4pm', '5pm', '6pm', '7pm', '8pm', '9pm', '10pm']\nCOLOR_REVERSED = plt.cm.get_cmap(\"RdYlGn\").reversed()\n\n# Text for the first subplot\ntext1 = \"\"\"\nFig 1. On the Right Highlights \\n\nThe Typically Busiest Time of Day. \\n\nHeatMap outlining which Hourly \\n \nIntervals per week experience \\n\nThe most concurrent Calls \\n\n\\n \\n \nFig 2. On the Left Highlights \\n\nThe difference between the Agents \\n\nScheduled and the Agents needed \\n \nTo answer the Average call volume \\n \nfor the Hourly Interval at the 75th \\n\npercentile \\n\n\"\"\"\n\n# Create a figure with a specified layout and size\nfig = plt.figure(figsize=(20, 40))\ngs = gridspec.GridSpec(4, 2)\n\n# Subplot 1: HeatMap Concurrent Calls per Hour\nax1 = fig.add_subplot(gs[0, 0])\n\nim1, cbar1 = heatmap(heatmap_data, CONCURRENT_HOURS, CONCURRENT_WEEKS, ax=ax1,\n                     cmap=COLOR_REVERSED, cbarlabel=\"Concurrent Calls\", vmin=0, cbar_kw={'shrink': 0.5})\ntexts1 = annotate_heatmap(im1, size=15)\nax1.set_title('Concurrent Calls at the 75th Percentile ', fontdict=titles_dict)\nax1.tick_params(axis='both', which='both', labelsize=15)\ncbar1.set_label(\"Calls per Hour\", fontdict=sub_title_dict)\ncbar1.ax.tick_params(axis='both', labelsize=15)\n\n# Subplot 2: HeatMap Actual Agents Delta\nax2 = fig.add_subplot(gs[0, 1])\nim2, cbar2 = heatmap(heatmap_delta_data, CONCURRENT_HOURS, CONCURRENT_WEEKS, ax=ax2,\n                     cmap=COLOR_REVERSED, cbarlabel=\"Concurrent Calls\", vmin=0, cbar_kw={'shrink': 0.5})\ntexts2 = annotate_heatmap(im2, size=15, threshold=0)\nax2.set_title('Delta: Actuals vs Needed Agents ', fontdict=titles_dict)\nax2.tick_params(axis='both', which='both', labelsize=15)\ncbar2.set_label(\"Calls per Hour\", fontdict=sub_title_dict)\ncbar2.ax.tick_params(axis='both', labelsize=15)\n\nbbox_1 = dict(boxstyle='round', facecolor='white', alpha=0.5)\nax2.text(1.3, 0.6, text1, ha='left', va='center', transform=ax2.transAxes, bbox=bbox_1, fontsize=12)\n\n# Subplot 3: Bar Plot for Call Arrivals\nax3 = fig.add_subplot(gs[1, 0:])\nax3.bar(x_ticks_arrivals, 'avg_interval', data=call_arrivals_15mins, color=color_brand[0], label='Actuals')\navg = call_arrivals_15mins['avg_interval'].mean()\navg_data_ = [avg for item in range(len(call_arrivals_15mins['avg_interval'].tolist()))]\nax3.plot(x_ticks_arrivals, avg_data_, color= color_brand[5], label='Average')\nax3.set_title('Average arrival patterns every 15 Mins', fontdict=titles_dict) \nax3.set_xlabel('Total Number of Calls') \nax3.set_ylabel('Hourly Interval') \nax3.tick_params(axis='both', which='both', labelsize=14, rotation=60)\nax3.legend()\n\n# Subplot 4: Horizontal Bar Plot for Handle Times - Agent\navg_weighted_ = handle_time_reason['weighted_aht'].sum()\n\nax4 = fig.add_subplot(gs[2, 0])\nbar_positions = range(len(handle_time_agent['agent_id']))\nhandle_time_agent = handle_time_agent.sort_values(by='total_handle')\nax4.barh(bar_positions, handle_time_agent['talk_time'], color=  color_brand[0], label='Talk Time')\nax4.barh(bar_positions, handle_time_agent['wrap_time'], left=handle_time_agent['talk_time'],color=  color_brand[1], label='Wrap Time')\nax4.axvline(x=avg_weighted_, color=  color_brand[4], linestyle='--', label='Target')\nax4.set_title('Average Hangle Times: Agent', fontdict=titles_dict)\nax4.set_xlabel('Seconds')\nax4.set_ylabel('Agent Number')\nax4.legend()\n\n# Subplot 5: Horizontl Bar Plot for Handle Times - Reason\nax5 = fig.add_subplot(gs[2, 1])\nhandle_time_reason = handle_time_reason.sort_values(by='total_handle')\nbar_positions = range(len(handle_time_reason['detail_reason']))\nax5.barh(bar_positions, handle_time_reason['talk_time'], color=  color_brand[0], label='Talk Time')\nax5.barh(bar_positions, handle_time_reason['wrap_time'], left=handle_time_reason['talk_time'],color=  color_brand[1], label='Wrap Time')\nax5.axvline(x=avg_weighted_, color=  color_brand[4], linestyle='--', label='Target')\nax5.set_title('Average Hangle Times: Reason', fontdict=titles_dict)\nax5.set_xlabel('Seconds')\nax5.set_ylabel('Call Reason Number')\nax5.legend()\n\n# Subplot 6: Line Plot for Call Volume Prediction\nax6 = fig.add_subplot(gs[3, 0])\nline_date = pd.to_datetime('2021-04-01')\nax6.plot('ds', 'y', data=projection_data, color=  color_brand[0], label='Actuals')\nax6.plot('ds', 'yhat', data=forecast_only_important[forecast_only_important['ds'] > line_date], color=  color_brand[3], label='Prediction')\nax6.scatter('ds', 'yhat_lower', data=forecast_only_important[forecast_only_important['ds'] > line_date], color=  color_brand[8], label='Prediction:Base')\nax6.scatter('ds', 'yhat_upper', data=forecast_only_important[forecast_only_important['ds'] > line_date], color=  color_brand[9], label='Prediction:Bold')\nax6.axvline(line_date, color=  color_brand[4], ls='--')\nax6.set_xlabel('Dates: 12 Months', fontdict=sub_title_dict)\nax6.set_ylabel('Call Volume', fontdict=sub_title_dict)\nax6.set_title('Predicting upcoming 12 Months Call Volume', fontdict=titles_dict)\nax6.legend()\n\n# Subplot 7: Weeky Total Calls Next 12 Months\n\n\nax7 = fig.add_subplot(gs[3, 1])\n# Plot the line\nline_1 = ax7.plot(x, y, label='Monthly $', color=  color_brand[3])\nbars = ax7.bar(x, y, color=  color_brand[1], width=2)\nax7.fill_between(x, y, color=  color_brand[1])\n\nscatters = ax7.bar(x_costs, y_costs, color=  color_brand[3])\nfor index, scatter in enumerate(scatters):\n    ax7.annotate(f'{labels_costs[index]}', xy=(scatter.get_x(), scatter.get_height() * 1.03), rotation=60, ha='left', va='bottom')\n\nax7.set_xlim(x[0])\nax7.set_ylim(y_costs[0] * 0.8, y_costs[-1] * 1.1)\n\nax7.set_xticks(x)\nax7.set_xticklabels(xlabels, rotation=60)\nax7.tick_params(axis='both', which='both', labelsize=14)\n\nax7.set_ylabel('Total Estimated Costs $', fontdict=sub_title_dict)\nax7.set_title('Total Projected Monthly Costs', fontdict=titles_dict)\n\nax8 = ax7.twinx()\nline_2 = ax8.plot(x, agents_projections_monthly['agents_needed_w_shrinkage'].tolist(), label=\"Agents Needed\", color=  color_brand[8])\n\nlines = line_1 + line_2\nlabels = [line.get_label() for line in lines]\nax7.legend(lines, labels, loc='center left', bbox_to_anchor=(1.05, 0.9))\n\nformatted_sum_12_month = '{:,}'.format(round(sum(y[:12]), 0))\n\ntext_7 = f'''\nRough Estimation of \\n\nfully loaded costs.\\n\nBased on US Bureau \\n\nof Labour's \\n\nMinimum living wage \\n\nin SF 2023. \\n\n\\n\nTotal cost 12 Months \\n\n= ${formatted_sum_12_month} \\n\n'''\nbbox_2 = dict(boxstyle='round', facecolor='white', alpha=0.5)\nax7.text(1.20, 0.6, text_7, ha='left', va='center', transform=ax7.transAxes, bbox=bbox_2, fontsize=12)\n\n# Adjust layout and display the figure\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-26T08:37:18.536179Z","iopub.execute_input":"2024-02-26T08:37:18.537074Z","iopub.status.idle":"2024-02-26T08:37:23.835203Z","shell.execute_reply.started":"2024-02-26T08:37:18.536877Z","shell.execute_reply":"2024-02-26T08:37:23.833924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project Conclusion üéâüìä\n\n#### Throughout this project, we embarked on a journey to gain deeper insights into the performance of our contact center. From transforming raw CSV data into visually insightful graphs and plots, we have uncovered valuable information that can inform decision-making.\n\n**Call Acceptance Rate and Service Level:** By calculating and visualizing the Call Acceptance Rate (CAR) and Service Level Agreement (SLA) over different time periods, we gained a clear understanding of our performance in terms of call handling efficiency.\n\n**Call Concurrency & Capacity:** Our exploration into call concurrency provided a nuanced understanding of our capacity to handle multiple calls simultaneously. This was further visualized through a Capacity Heat Map, shedding light on potential bottlenecks and areas for improvement.\n\n**Future Call Need:** Leveraging the power of Prophet's Time Series Forecasting Models, we created a simple yet effective projection model to predict future call volumes. This predictive capability equips us with the foresight to proactively manage resources and optimize our contact center's performance.\n\nIn conclusion, this project has not only provided us with valuable insights but has also equipped us with the tools and methodologies to continuously monitor and improve our contact center's performance. As we move forward, we will leverage these insights to drive strategic decision-making and enhance the overall customer experience.","metadata":{}}]}
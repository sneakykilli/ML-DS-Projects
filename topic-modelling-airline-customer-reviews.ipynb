{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7522116,"sourceType":"datasetVersion","datasetId":4379017},{"sourceId":7580346,"sourceType":"datasetVersion","datasetId":4365540},{"sourceId":7635864,"sourceType":"datasetVersion","datasetId":4431929}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/killianmcguinness/topic-modelling-airline-customer-reviews?scriptVersionId=166416017\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Topic Modeling using BERTopic to understand user comments\n\n## Objective:\nUsing a dataset comprised of user-generated comments for 5 prominent airlines, use various Topic Modelling strategies to understand what users think about each provider. We will use BERTopic for Topic Modelling and then use Cohere to summarize our findings into a scorecard for the airlines.\n\n## About The Data:\nThis dataset comprises user-generated reviews for five prominent airlines‚ÄîRyanAir, EasyJet, Singapore Air,Qatar & Emirates. The data was extracted from a comparison website where users share their experiences and rate service providers. The dataset spans a 12-month period from January 2023 to January 2024. \n\n**Period:**\nJanuary 28, 2023, to January 28, 2024.\n\nNote - You can visit my [GitHub](https://github.com/sneakykilli) to see how I comprised this Dataset\n\n\n# Action Plan\n| Step | Task                                   | Objective                                               | Details                                            |\n|------|----------------------------------------|---------------------------------------------------------|----------------------------------------------------|\n| 1    | Data Preprocessing                     | Ensure Dataframe is in the correct format for the Model.        | - Import Libraries. <br> - Examine data types, missing values, and basic statistics. <br> - Convert Date from strings to Datetime to ensure accurate time-based analysis. |\n| 2    | BERTopic Experimentation               | Figure out the Best BERTopic Configuration.             | - Make Agnostic col, replace reference to specific Airline with a generic stand-in. <br> - Pre-calculate Embeddings & Save for better Runtime. <br> - Build 5 Different Pipelines using Different Hyper Parameter Tuning. |\n| 3    | Deep Dive into Clustering               | Fine-tune the Clustering Approach & Evaluate Results.       | - Visualize Topic Hierarchy, Add custom Labels. <br> - Visualize Top 10 Feedback Topics. <br> - Compare the Top 10 Topics per Airline to Generic Topics |\n| 4    | Train Model on Airline Data               | Further Fine-Tune the Modelling Approach above & Train on Airline-specific Data.       | - Train on separate Airline Datasets, using Cohere as the Representation Model for EasyJet & Qatar. <br> - Save Models Locally & to Hugging Faces to Conserve Compute Power. <br> - Use Cohere API to Generate a Summary of all Representative Docs per Topic as well as an Intro Summary |\n","metadata":{}},{"cell_type":"markdown","source":"[](http://)","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Data Preparation üõ†Ô∏è \n### Read in CSV data & make it easier to work with.<br>\n\n**1. Import Key Libraries:**\n- Import necessary libraries, there are quite a few! \n\n**2. Color Palette:**\n- Import a Color Palette & Style Guide for Graphs. \n\n**3. Data Loading & Cleaning:**\n- Read in Dataset CSV and convert to Pandas Dataframe.\n- Name the Cols something Meaningful. \n- Date Col a bit messy, convert it from String to Datetime object üìÖ","metadata":{}},{"cell_type":"code","source":"!pip install cohere;\n!pip install bertopic sentence-transformers cohere;\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport datetime\nimport time\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom bertopic import BERTopic\nfrom umap import UMAP\nfrom datasets import load_dataset\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom hdbscan import HDBSCAN\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, Cohere","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-01T09:58:56.316852Z","iopub.execute_input":"2024-03-01T09:58:56.317289Z","iopub.status.idle":"2024-03-01T09:59:20.768332Z","shell.execute_reply.started":"2024-03-01T09:58:56.317257Z","shell.execute_reply":"2024-03-01T09:59:20.766873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Color Pallette & Font Dicts, Will use later to keep graphs & plots looking similar. \ncolor_pal = sns.color_palette(\"tab20c\")\ncolor_brand = ['#ABE3C4', '#E7F7EE', '#5F8778', '#85B59E', '#E3ABCA', '#FFCCCC', '#F18C72', '#6BAED6', '#9ECAE1', '#D9D9D9']\nplt.style.use('fivethirtyeight')\n\ntitles_dict = {'fontsize': 28,\n 'fontweight': 25,\n 'color':   color_brand[2]}\n\nsub_title_dict = {'fontsize': 20,\n 'fontweight': 18,\n 'color':   color_brand[2]}\n\nfig_text_dict = {\n    'color':   color_brand[2], \n}\n\ntextprops={'color': color_brand[2], 'fontsize':8}","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:20.773552Z","iopub.execute_input":"2024-03-01T09:59:20.774021Z","iopub.status.idle":"2024-03-01T09:59:20.786626Z","shell.execute_reply.started":"2024-03-01T09:59:20.773984Z","shell.execute_reply":"2024-03-01T09:59:20.785218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Dataframe & Name cols. Create a var companies containing List of the Airlines. \ndf = pd.read_csv('/kaggle/input/user-comments-travel-companies/airlines_12_months.csv')\ndf.columns = ['company', 'date', 'comment', 'star']\ncompanies = df['company'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:20.787821Z","iopub.execute_input":"2024-03-01T09:59:20.788156Z","iopub.status.idle":"2024-03-01T09:59:20.888289Z","shell.execute_reply.started":"2024-03-01T09:59:20.788115Z","shell.execute_reply":"2024-03-01T09:59:20.88722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Deal with Data format issues in Original DF\n\n# Extract unique date values from the 'date' column\nunique_date_values = df['date'].unique().tolist()\n\n# Identify dates with 'ago' in them\nago_date_values = [item for item in unique_date_values if 'ago' in item]\n\n# Fixed list of dates without 'ago'\nago_fixed_list = ['Jan 27, 2024', 'Jan 26, 2024', 'Jan 28, 2024', 'Jan 28, 2024', 'Jan 25, 2024', 'Jan 24, 2024',\n                  'Jan 27, 2024', 'Jan 27, 2024', 'Jan 22, 2024', 'Jan 23, 2024', 'Jan 21, 2024', 'Jan 23, 2024',\n                  'Jan 21, 2024', 'Jan 24, 2024', 'Jan 28, 2024', 'Jan 26, 2024', 'Jan 27, 2024', 'Jan 28, 2024',\n                  'Jan 25, 2024']\n\n# Create a dictionary mapping 'ago' dates to fixed dates\nago_fixed_dict = dict(zip(ago_date_values, ago_fixed_list))\n\n# Identify dates with 'Updated' but without 'ago'\nupdated_date_values = [item for item in unique_date_values if 'Updated' in item and 'ago' not in item]\n\n# Extract fixed dates from 'Updated' dates\nfixed_updated_list = [item.split('Updated ')[1] for item in updated_date_values]\n\n# Create a dictionary mapping 'Updated' dates to fixed dates\nupdated_fixed_dict = dict(zip(updated_date_values, fixed_updated_list))\n\n# Update the 'ago' dictionary with the 'Updated' dictionary\nago_fixed_dict.update(updated_fixed_dict)\n\n# Function to update values based on the created dictionary\ndef update_value_dict_map(my_value, my_dict):\n    for key, value in my_dict.items():\n        if key == my_value:\n            return value\n    return my_value\n\n# Apply the update function to the 'date' column to get 'updated_dates'\ndf['updated_dates'] = df['date'].apply(lambda x: update_value_dict_map(x, ago_fixed_dict))\n\n# Convert 'updated_dates' to datetime format\ndf['updated_dates'] = pd.to_datetime(df['updated_dates'], format='%b %d, %Y')\n\n# Save the updated DataFrame to a CSV file\ndf.to_csv('updated_airlines_12_months.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:20.889689Z","iopub.execute_input":"2024-03-01T09:59:20.890021Z","iopub.status.idle":"2024-03-01T09:59:21.081364Z","shell.execute_reply.started":"2024-03-01T09:59:20.889991Z","shell.execute_reply":"2024-03-01T09:59:21.080185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: BERTopic Model Testing üîç \n### Objective: Using our Entire Dataset & agnostic Comments play around with different configurations of BERTopic pipelines to understand How it performs on our Data. <br>\n\n**1. Make Comments Agnostic:**\n- Add a column to our DataFrame that makes the comment text airline agnostic, i.e., replace easyjet, ryanair with killiair. üîÑ\n\n**2. Save Embeddings:**\n- To avoid the process of vector embedding each time we train a model, we're going to save our embeddings and reuse them for each iteration. üîÑ\n\n**3. Test:**\n- We're going to test 5 different iterations of BERTopic modeling approaches to understand which setup works best on our data.\n    1. No Fine at all\n    2. Stop words Removed - Common words like \"the\" or \"and\" are often removed before topic modeling as they don't carry much meaning. \n    3. Seed Words - Pre-defined words or phrases used to guide topic modeling and improve the quality of topics identified.\n    4. Guiding - Providing additional information or constraints to help the model find more accurate and relevant topics.\n    5. Clustering using hdbscan_model - Grouping similar documents or data points together to identify distinct topics or themes.\n\n**4. Visualize:**\n- Using BERTopic's visualise attribute, visualize the results. üìä","metadata":{}},{"cell_type":"code","source":"# Add a Col containing Comments but with no reference to Specific Airlines\n\n# Define a list of airline names and their proxies\ncarrier_names_proxies = ['Ryanair', 'Easyjet', 'Singapore', 'Singapore Airlines', 'Singapore Air', 'Qatar', 'Qatar Air', 'Qatar Airlines', 'Emirates', 'Airline']\n\n# Function to replace airline names with a generic term\ndef agnostic_comment(comment):\n    for word in carrier_names_proxies:\n        comment = comment.casefold().replace(word.casefold(), 'killiair')\n    return comment\n\n# Apply the function to the 'comment' column to create a new 'comment_agnostic' column\ndf['comment_agnostic'] = df['comment'].apply(agnostic_comment)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:21.08502Z","iopub.execute_input":"2024-03-01T09:59:21.085537Z","iopub.status.idle":"2024-03-01T09:59:21.323124Z","shell.execute_reply.started":"2024-03-01T09:59:21.08549Z","shell.execute_reply":"2024-03-01T09:59:21.322067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save embeddings of the original comments in a DataFrame\n# NB UNCOMMENT TO RUN !!!!\n\n# from sentence_transformers import SentenceTransformer\n\n# sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# embeddings_agnostic = sentence_model.encode(df['comment_agnostic'], show_progress_bar=True)\n\n# embeddings_specific = sentence_model.encode(df['comment'], show_progress_bar=True)\n\n# np.save('/kaggle/working/embeddings_agnostic.npy', embeddings_agnostic)\n# np.save('/kaggle/working/embeddings_specific.npy', embeddings_specific)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:21.324453Z","iopub.execute_input":"2024-03-01T09:59:21.324818Z","iopub.status.idle":"2024-03-01T09:59:21.330185Z","shell.execute_reply.started":"2024-03-01T09:59:21.324788Z","shell.execute_reply":"2024-03-01T09:59:21.328829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aim: Read in the saved embeddings & get them in a format that BERTopic can use for training.\n\n# embeddings associated with the agnostic version of comments (i.e. replacing any company-specific mention with killiair)\nembeddings_agnostic = np.load('/kaggle/input/embeddings-airlines/embeddings_agnostic.npy', allow_pickle=True)\ndf['embeddings_agnostic'] = list(embeddings_agnostic)\n\n# We will use embeddings for specific companies a little later!\nembeddings_specific = np.load('/kaggle/input/embeddings-airlines/embeddings_specific.npy', allow_pickle=True)\n\nryanair_loc_ = df.loc[df['company'] == 'www.ryanair.com'].index\neasy_loc_ = df.loc[df['company'] == 'www.easyjet.com'].index\nsing_loc_ = df.loc[df['company'] == 'www.singaporeair.com'].index\nqatar_loc_ = df.loc[df['company'] == 'www.qatarairways.com'].index\nemi_loc_ = df.loc[df['company'] == 'www.emirates.com'].index\n\nemb_ryan = embeddings_specific[ryanair_loc_.min():ryanair_loc_.max() + 1]\nemb_easy = embeddings_specific[easy_loc_.min():easy_loc_.max() + 1]\nemb_sing = embeddings_specific[sing_loc_.min():sing_loc_.max() + 1]\nemb_qatar = embeddings_specific[qatar_loc_.min():qatar_loc_.max() + 1]\nemb_emi = embeddings_specific[emi_loc_.min():emi_loc_.max() + 1]","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:21.332181Z","iopub.execute_input":"2024-03-01T09:59:21.332953Z","iopub.status.idle":"2024-03-01T09:59:21.372661Z","shell.execute_reply.started":"2024-03-01T09:59:21.332903Z","shell.execute_reply":"2024-03-01T09:59:21.371661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [Pipeline 1] Train model with no fine tuning\n\n# Define the embedding model to be used\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# Initialize BERTopic model with the specified embedding model\ntopic_model_a = BERTopic(embedding_model=embedding_model)\n\n# Fit the BERTopic model to the agnostic comments and their embeddings\ntopics_a, probs_a = topic_model_a.fit_transform(df['comment_agnostic'], embeddings_agnostic)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:21.374316Z","iopub.execute_input":"2024-03-01T09:59:21.374712Z","iopub.status.idle":"2024-03-01T09:59:33.387982Z","shell.execute_reply.started":"2024-03-01T09:59:21.374678Z","shell.execute_reply":"2024-03-01T09:59:33.386753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [PIPELINE 2] TRAIN MODEL ON COMMENTS WITH STOP WORDS REMOVED\n\nvectorizer_model = CountVectorizer(stop_words=\"english\")\ntopic_model_b = BERTopic(\n                        vectorizer_model=vectorizer_model, \n                        embedding_model = embedding_model\n                        )\ntopics_b, probs_b = topic_model_b.fit_transform(df['comment_agnostic'], embeddings_agnostic)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:33.389831Z","iopub.execute_input":"2024-03-01T09:59:33.390206Z","iopub.status.idle":"2024-03-01T09:59:45.516923Z","shell.execute_reply.started":"2024-03-01T09:59:33.390176Z","shell.execute_reply":"2024-03-01T09:59:45.515676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# [Pipeline 3] Train a model using seed words\n\n# Define a list of seed words to use for topic modeling\nseed_words = [\n    'reservation', 'booking', 'online booking', 'ticket purchase', 'booking system',\n    'service quality', 'customer support', 'assistance', 'helpdesk', 'service experience',\n    'flight delays', 'arrival delays', 'departure delays', 'delayed flights', 'schedule disruptions',\n    'flight cancellations', 'canceled flights', 'cancellation policy', 'itinerary changes', 'canceled services',\n    'lost baggage', 'missing luggage', 'baggage claim', 'lost items', 'luggage recovery',\n    'check-in experience', 'check-in process', 'online check-in', 'check-in counter', 'boarding pass',\n    'entertainment options', 'in-flight movies', 'onboard entertainment', 'streaming services', 'in-flight media',\n    'flight attendants', 'cabin crew', 'ground staff', 'crew behavior', 'staff professionalism',\n    'onboard meals', 'food quality', 'meal options', 'catering service', 'in-flight dining'\n]\n\n# Initialize a ClassTfidfTransformer with the defined seed words\nctfidf_model = ClassTfidfTransformer(\n    seed_words=seed_words, \n    seed_multiplier=2,\n)\n\n# Initialize a BERTopic model with the defined ctfidf_model, vectorizer_model, and embedding_model\ntopic_model_c = BERTopic(\n    ctfidf_model=ctfidf_model,\n    vectorizer_model=vectorizer_model, \n    embedding_model=embedding_model,\n)\n\n# Fit the BERTopic model to the agnostic comments and their embeddings\ntopics_c, probs_c = topic_model_c.fit_transform(df['comment_agnostic'], embeddings_agnostic)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:45.518734Z","iopub.execute_input":"2024-03-01T09:59:45.522577Z","iopub.status.idle":"2024-03-01T09:59:57.821734Z","shell.execute_reply.started":"2024-03-01T09:59:45.522529Z","shell.execute_reply":"2024-03-01T09:59:57.82064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aim: [Pipeline 4] Train a model using seed word guiding\n\n# Define a list of lists of seed words to use for topic modeling\nseed_words_list = [\n    ['reservation', 'booking', 'online booking', 'ticket purchase', 'booking system'],\n    ['service quality', 'customer support', 'assistance', 'helpdesk', 'service experience'],\n    ['flight delays', 'arrival delays', 'departure delays', 'delayed flights', 'schedule disruptions'],\n    ['flight cancellations', 'canceled flights', 'cancellation policy', 'itinerary changes', 'canceled services'],\n    ['lost baggage', 'missing luggage', 'baggage claim', 'lost items', 'luggage recovery'],\n    ['check-in experience', 'check-in process', 'online check-in', 'check-in counter', 'boarding pass'],\n    ['entertainment options', 'in-flight movies', 'onboard entertainment', 'streaming services', 'in-flight media'],\n    ['flight attendants', 'cabin crew', 'ground staff', 'crew behavior', 'staff professionalism'],\n    ['onboard meals', 'food quality', 'meal options', 'catering service', 'in-flight dining']\n]\n\n# Initialize a BERTopic model with the defined seed_topic_list, vectorizer_model, and calculate_probabilities\ntopic_model_d = BERTopic(\n    seed_topic_list=seed_words_list,\n    vectorizer_model=vectorizer_model,\n    calculate_probabilities=True,\n)\n\n# Fit the BERTopic model to the agnostic comments and their embeddings\ntopics_d, probs_d = topic_model_d.fit_transform(df['comment_agnostic'], embeddings_agnostic)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T09:59:57.823299Z","iopub.execute_input":"2024-03-01T09:59:57.823782Z","iopub.status.idle":"2024-03-01T10:00:10.934952Z","shell.execute_reply.started":"2024-03-01T09:59:57.823738Z","shell.execute_reply":"2024-03-01T10:00:10.933726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aim: [Pipeline 5] Build a model using clustering\n\n# Initialize an HDBSCAN model with specified parameters\nhdbscan_model = HDBSCAN(\n    min_cluster_size=10, \n    metric='euclidean', \n    cluster_selection_method='eom', \n    prediction_data=True\n)\n\n# Initialize a BERTopic model with the defined hdbscan_model, vectorizer_model, and embedding_model\ntopic_model_e = BERTopic(\n    hdbscan_model=hdbscan_model, \n    vectorizer_model=vectorizer_model,\n    embedding_model=embedding_model\n)\n\n# Fit the BERTopic model to the agnostic comments and their embeddings\ntopics_e, probs_e = topic_model_e.fit_transform(df['comment_agnostic'], embeddings_agnostic)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:10.936571Z","iopub.execute_input":"2024-03-01T10:00:10.937027Z","iopub.status.idle":"2024-03-01T10:00:23.297582Z","shell.execute_reply.started":"2024-03-01T10:00:10.936982Z","shell.execute_reply":"2024-03-01T10:00:23.296452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the models trained above\n\n# Visualize the topics for each model\nvisualise_a = topic_model_a.visualize_topics()\nvisualise_b = topic_model_b.visualize_topics()\nvisualise_c = topic_model_c.visualize_topics()\nvisualise_d = topic_model_d.visualize_topics()\nvisualise_e = topic_model_e.visualize_topics()\n\n# Create a subplot with 3 rows and 2 columns\nfig = make_subplots(rows=3, cols=2, subplot_titles=['No Fine Tuning', 'No Stopwords', 'Seeds', 'Guiding', 'Clustering'])\n\n# Add traces for each visualization to the subplot\nfor trace in visualise_a['data']:\n    fig.add_trace(trace, row=1, col=1)\n    \nfor trace in visualise_b['data']:\n    fig.add_trace(trace, row=1, col=2)\n    \nfor trace in visualise_c['data']:\n    fig.add_trace(trace, row=2, col=1)\n    \nfor trace in visualise_d['data']:\n    fig.add_trace(trace, row=2, col=2)\n    \nfor trace in visualise_e['data']:\n    fig.add_trace(trace, row=3, col=1)\n    \n# Update the layout of the subplot\nfig.update_layout(title_text='Fine Tuning BERTopic Topic Modelling', title_x=0.5)\n    \n# Show the figure\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:23.299179Z","iopub.execute_input":"2024-03-01T10:00:23.299547Z","iopub.status.idle":"2024-03-01T10:00:29.173243Z","shell.execute_reply.started":"2024-03-01T10:00:23.299514Z","shell.execute_reply":"2024-03-01T10:00:29.172035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Deepdive into Clustering üî¨\n### Clustering approach seems to yield good results, let's apply some post-training fine-tuning & see what findings we can extrapolate. <br>\n\n**1. View the Cluster Hierarchy:**\n- Get a big picture view of each of the topics our model has detected. üåê\n\n**2. Apply Custom Labels & Visualise:**\n- BERTopic applied topic representations which describe what the topic is about. We want to apply some custom labels to make it easier to read.\n- We want to reduce the number of topics from 50+ to about 10 & visualize the distribution in a simple pie chart. üìä\n\n**3. Compare Topic Breakdown across Airlines:**\n- At this point, we can compare the distribution of representative topics between specific airlines. We'll build a stacked bar chart that compares the top 10 topic reasons and their relative distribution among the different airlines. üìä","metadata":{}},{"cell_type":"code","source":"# Visualize the hierarchical clustering\n\n# Visualize the hierarchical clustering for the clustering model\ntopic_model_e.visualize_hierarchy(title='Clustering')","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:29.177965Z","iopub.execute_input":"2024-03-01T10:00:29.178388Z","iopub.status.idle":"2024-03-01T10:00:29.2602Z","shell.execute_reply.started":"2024-03-01T10:00:29.178345Z","shell.execute_reply":"2024-03-01T10:00:29.258613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply custom labels to the topics\n\n# Define a dictionary with custom labels for the topics\nmy_custom_labels = {\n    0: 'Bagge Policy', \n    1: 'Seat Allocation',\n    2: 'Refund request (Delay/Calcel)', \n    3: 'Bad Experience', \n    4: 'Poor Customer Service', \n    5: 'Great Experience', \n    6: 'Ticket Change Policy', \n    7: 'Customer Service Bot', \n    8: 'Sueing / Legal Action', \n    9: 'Good Customer Service'\n}\n\ntopic_model_e.set_topic_labels(my_custom_labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:29.261913Z","iopub.execute_input":"2024-03-01T10:00:29.262309Z","iopub.status.idle":"2024-03-01T10:00:29.273754Z","shell.execute_reply.started":"2024-03-01T10:00:29.262276Z","shell.execute_reply":"2024-03-01T10:00:29.272562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reduce the number of topics to 11 and visualize the top 10 feedback for airlines\n\ntopic_model_e.reduce_topics(df['comment_agnostic'], nr_topics=11)\ntopic_model_e.set_topic_labels(my_custom_labels)\n\n# Get information about the clusters\nclusters_info = topic_model_e.get_topic_info()\n\n# Drop the cluster with label -1\nclusters_info.drop(0, inplace=True)\n\n# Extract labels and sizes for the pie chart\ntry: \n    labels = clusters_info['CustomName']\nexcept KeyError:\n    topic_model_e.set_topic_labels(my_custom_labels)\n    labels = clusters_info['CustomName']\n\nsizes = clusters_info['Count']\n\n# Create a pie chart of the top 10 feedback for airlines\nplt.figure(figsize=(8, 8))\nplt.pie(sizes, autopct='', startangle=140, colors=color_brand)\nplt.legend(labels, loc='center left', bbox_to_anchor=(1, 0.5), title='Cluster Names')\nplt.axis('equal') \nplt.title('Top 10 Feedback Topic for Airlines Generally', fontdict=titles_dict)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:29.275374Z","iopub.execute_input":"2024-03-01T10:00:29.275961Z","iopub.status.idle":"2024-03-01T10:00:30.915325Z","shell.execute_reply.started":"2024-03-01T10:00:29.27593Z","shell.execute_reply":"2024-03-01T10:00:30.914191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build stacked bars to compare feedback between different companies\n\n# Reduce the number of topics to 11\ntopic_model_e.reduce_topics(df['comment_agnostic'], nr_topics=11)\n\n# Get information about the clusters\ninfo_clustering = topic_model_e.get_document_info(df['comment_agnostic'])\n\n# Add a column to the dataframe to store the cluster labels\ndf['clustering_reps'] = info_clustering['CustomName']\n\n# Get the count of documents for each cluster\ndf_counts_all = df.groupby('clustering_reps').size().reset_index(name='count')\ndf_counts_all = df_counts_all.drop(df_counts_all.index[0])\ndf_counts_all = df_counts_all.sort_values(by='clustering_reps')\ndf_counts_all['%'] = (df_counts_all['count'] / df_counts_all['count'].sum()) * 100\n\n# Initialize lists to store data for the stacked bars\nstacked_data = {}\nairlines = (\n    \"All Airlines\", \n    \"RyanAir\", \n    \"EasyJet\",\n    \"Singapore Air\", \n    \"Qatar Air\", \n    \"Emirates\"\n)\n\n# Get the percentage of documents for each cluster for each airline\nfor company in companies:\n    df_ = df[df['company'] == company]\n    df_ = df_.groupby('clustering_reps').size().reset_index(name='count')\n    df_ = df_.drop(df_.index[0])\n    df_ = df_.sort_values(by='clustering_reps')\n    df_['%'] = (df_['count'] / df_['count'].sum()) * 100\n    df_counts_all[company] = df_['%']\n\n# Get the category names for the stacked bars\nclustering_reps = df_counts_all['clustering_reps'].tolist()\n\n# Create the stacked bars\nfig, ax = plt.subplots(figsize=(12, 8))\nbottom = np.zeros(6)\nwidth = 0.5\n\nfor index, rep in enumerate(clustering_reps):\n    data_point = np.array(df_counts_all.iloc[index].tolist()[2:])\n    p = ax.bar(airlines, data_point, width, label=rep, bottom=bottom, color=color_brand[index % len(color_brand)])\n    bottom += data_point\n\nax.set_title(\"\\n % Share of Each Feedback Category Per Airline \\n\", fontdict=titles_dict)\nax.legend(loc='upper left', bbox_to_anchor=(1, 1), bbox_transform=fig.transFigure, title='\\n Feedback Category \\n')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:30.916696Z","iopub.execute_input":"2024-03-01T10:00:30.917019Z","iopub.status.idle":"2024-03-01T10:00:31.714905Z","shell.execute_reply.started":"2024-03-01T10:00:30.916989Z","shell.execute_reply":"2024-03-01T10:00:31.713646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Step 4: Apply Model to Airline Specific Datasets üìä\n#### It looks like Iteration 5, clustering works well with our Generic Dataset. We'll use this approach to train Model on Airline Specific Data Sets with some additional Fine tuning to account for corpus size. <br>\n\n**1. Train on Airline Specific Data:**\n- Using what we learned above, we'll deploy the model to our airline-specific datasets. We'll take some additional steps to fine-tune by reducing the minimum cluster size to account for the reduced corpus size.\n- For two of our datasets, we'll get the topic representation using Cohere, an external LLM Tool (Note: we've limited to two datasets as I have some API Limitations :D)\n\n**2. Save The Model Results Locally & To Hugging Faces Hub:**\n- Save model results locally to avoid retraining & hitting API limits.\n- Save the model to Hugging Faces to use in the future with expanded datasets.\n\n**3. Using Cohere & Results from Model Build Text Insights:**\n- Build a function that takes in the most representative docs per topic and returns a summary of the comments.\n- Build a function that reviews the most representative docs and suggests a topic name.\n- Build a function that creates a stacked bar chart summarizing the most common topic per month. üìä","metadata":{}},{"cell_type":"code","source":"# Train Model on General Approach of Pipeline 5 but reducing Min Cluster Size\n# NB Uncomment to Run! If using Cohere API Limit may apply \n\n# Import necessary libraries\n# from bertopic.representation import KeyBERTInspired\n# from bertopic.representation import MaximalMarginalRelevance\n# from sklearn.feature_extraction.text import CountVectorizer\n# from hdbscan import HDBSCAN\n\n# # Install Cohere\n# ! pip install cohere;\n\n# import cohere\n# from bertopic.representation import Cohere\n\n# # Get comments for specific airlines\n# doc_ryan = df['comment'][df['company'] == 'www.ryanair.com']\n# doc_easy = df['comment'][df['company'] == 'www.easyjet.com']\n# doc_sing = df['comment'][df['company'] == 'www.singaporeair.com']\n# doc_qatar = df['comment'][df['company'] == 'www.qatarairways.com']\n# doc_emi = df['comment'][df['company'] == 'www.emirates.com']\n\n# # Initialize representation models\n# representation_model = KeyBERTInspired()\n\n# my_key = [ADD_YOUR_APIKEY]\n# co = cohere.Client(my_key)\n# representation_model_cohere = Cohere(co, delay_in_seconds=12, model='command')\n# representation_model_bert = KeyBERTInspired()\n# representation_model_maximal = MaximalMarginalRelevance(diversity=0.3)\n\n# # Initialize vectorizer model\n# vectorizer_model = CountVectorizer(stop_words=\"english\")\n\n# # Initialize HDBSCAN model\n# hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n\n# # Train the Ryanair model\n# ryan_air_model = BERTopic(\n#     min_topic_size = 5,\n#     hdbscan_model=hdbscan_model, \n#     vectorizer_model=vectorizer_model, \n#     embedding_model=\"all-MiniLM-L6-v2\",\n#     representation_model=representation_model_bert)\n\n# topics_ryan, probs_ryan = ryan_air_model.fit_transform(doc_ryan, emb_ryan)\n\n# # Train the Easyjet model\n# easyjet_model = BERTopic(\n#     min_topic_size = 5,\n#     hdbscan_model=hdbscan_model, \n#     vectorizer_model=vectorizer_model, \n#     embedding_model=\"all-MiniLM-L6-v2\", \n#     representation_model=representation_model_cohere)\n\n# topics_easy, probs_easy = easyjet_model.fit_transform(doc_easy, emb_easy)\n\n# # Train the Singapore model\n# singapore_model = BERTopic(\n#     min_topic_size = 5, \n#     hdbscan_model=hdbscan_model, \n#     vectorizer_model=vectorizer_model, \n#     embedding_model=\"all-MiniLM-L6-v2\", \n#     representation_model=representation_model_bert)\n\n# topics_sing, probs_sing = singapore_model.fit_transform(doc_sing, emb_sing)\n\n# # Train the Qatar model\n# qatar_model = BERTopic(\n#     min_topic_size = 5, \n#     hdbscan_model=hdbscan_model, \n#     vectorizer_model=vectorizer_model, \n#     embedding_model=\"all-MiniLM-L6-v2\", \n#     representation_model=representation_model_cohere)\n\n# topics_qatar, probs_qatar = qatar_model.fit_transform(doc_qatar, emb_qatar)\n\n# # Train the Emirates model\n# emirates_model = BERTopic(\n#     min_topic_size = 5,\n#     hdbscan_model=hdbscan_model, \n#     vectorizer_model=vectorizer_model, \n#     embedding_model=\"all-MiniLM-L6-v2\", \n#     representation_model=representation_model_maximal)\n\n# topics_emi, probs_emi = emirates_model.fit_transform(doc_emi, emb_emi)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:31.716332Z","iopub.execute_input":"2024-03-01T10:00:31.716716Z","iopub.status.idle":"2024-03-01T10:00:31.725086Z","shell.execute_reply.started":"2024-03-01T10:00:31.716684Z","shell.execute_reply":"2024-03-01T10:00:31.723567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # reate & save dataframes from the different models above, because we have API limits\n\n# # Define the columns to add to the dataframes\n# cols_to_add = ['Topic', 'Name', 'Representation', 'Representative_Docs', 'Top_n_words']\n\n# # Define the models and documents for each company\n# model_holder = [ryan_air_model, easyjet_model, singapore_model, qatar_model, emirates_model]\n# doc_holder = [doc_ryan, doc_easy, doc_sing, doc_qatar, doc_emi]\n\n# # Loop through each company and create a dataframe with the results\n# for index, company in enumerate(companies):\n#     # Get the documents for the current company\n#     df_ = df[df['company'] == company]\n#     df_.reset_index(drop=True, inplace=True)\n    \n#     # Get the results for the current company\n#     results_ = model_holder[index].get_document_info(doc_holder[index])[cols_to_add]\n    \n#     # Add the results to the dataframe\n#     df_[cols_to_add] = results_[cols_to_add]\n    \n#     # Save the dataframe to a csv file\n#     df_.to_csv(f'/kaggle/working/df_results_{company}.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:31.72657Z","iopub.execute_input":"2024-03-01T10:00:31.726945Z","iopub.status.idle":"2024-03-01T10:00:31.74238Z","shell.execute_reply.started":"2024-03-01T10:00:31.726915Z","shell.execute_reply":"2024-03-01T10:00:31.740906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Save Models to Hugging Face Hub\nfrom huggingface_hub import login\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:31.743884Z","iopub.execute_input":"2024-03-01T10:00:31.744523Z","iopub.status.idle":"2024-03-01T10:00:31.790154Z","shell.execute_reply.started":"2024-03-01T10:00:31.744483Z","shell.execute_reply":"2024-03-01T10:00:31.788904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ryan_air_model.push_to_hf_hub(\n#     repo_id=\"sneakykilli/Ryanair_BERTopic\",\n#     save_ctfidf=True\n# )\n\n# easyjet_model.push_to_hf_hub(\n#     repo_id=\"sneakykilli/Easyjet_BERTopic\",\n#     save_ctfidf=True\n# )\n\n# singapore_model.push_to_hf_hub(\n#     repo_id=\"sneakykilli/Singapore_BERTopic\",\n#     save_ctfidf=True\n# )\n\n# qatar_model.push_to_hf_hub(\n#     repo_id=\"sneakykilli/Qatar_BERTopic\",\n#     save_ctfidf=True\n# )\n\n# emirates_model.push_to_hf_hub(\n#     repo_id=\"sneakykilli/Emirates_BERTopic\",\n#     save_ctfidf=True\n# )","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:31.791501Z","iopub.execute_input":"2024-03-01T10:00:31.791836Z","iopub.status.idle":"2024-03-01T10:00:31.798029Z","shell.execute_reply.started":"2024-03-01T10:00:31.791806Z","shell.execute_reply":"2024-03-01T10:00:31.796729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aim: Read in dataframes of results of clustering per airlines\ndf_ryan = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_results_www.ryanair.com.csv')\ndf_easy = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_results_www.easyjet.com.csv')\ndf_singapore = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_results_www.singaporeair.com.csv')\ndf_qatar = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_results_www.qatarairways.com.csv')\ndf_emirates = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_results_www.emirates.com.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:31.800067Z","iopub.execute_input":"2024-03-01T10:00:31.800725Z","iopub.status.idle":"2024-03-01T10:00:33.11931Z","shell.execute_reply.started":"2024-03-01T10:00:31.800684Z","shell.execute_reply":"2024-03-01T10:00:33.118051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aim: Create a function that takes a dataframe as an input and summarizes the representative docs\n\ndef generate_document_summaries(df):\n    # Initialize an empty list to store the summaries\n    holder = []\n    \n    # Drop duplicates from the dataframe based on the 'Representative_Docs' column\n    unique_df = df.drop_duplicates(subset=['Representative_Docs'])\n    \n    # Extract the unique representative docs as a list\n    example_docs = unique_df['Representative_Docs'].tolist()\n    \n    # Initialize a Cohere client with your API key\n    co = cohere.Client(my_api_key)\n\n    # Loop through each representative doc and summarize it\n    for item in example_docs:\n        try:\n            # Use the Cohere API to generate a summary\n            response = co.summarize(text=item, length='medium', extractiveness='medium', temperature=5, additional_command=\"focusing on what can be improved, end the summary with a Next Action suggestion\")\n            \n            # Pause for 10 seconds between requests to avoid rate limiting\n            time.sleep(10)\n            \n            # Append the summary to the holder list\n            holder.append(response)\n        except Exception as e:\n            # Print any errors that occur during processing\n            print(f\"Error processing document: {e}\")\n            continue\n    \n    # Return the list of summaries\n    return holder","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.121062Z","iopub.execute_input":"2024-03-01T10:00:33.121483Z","iopub.status.idle":"2024-03-01T10:00:33.130456Z","shell.execute_reply.started":"2024-03-01T10:00:33.121448Z","shell.execute_reply":"2024-03-01T10:00:33.128971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create summaries of all representative docs and save to a dataframe so we don't re-run\n# # NB Uncomment to Run, Update API Key & Beware of API Limits!\n\n# # Generate summaries for the representative documents of each airline\n# ryanair_summaries = generate_document_summaries(df_ryan)\n# easyjet_summaries = generate_document_summaries(df_easy)\n# qatarairways_summaries = generate_document_summaries(df_qatar)\n# singaporeair_summaries = generate_document_summaries(df_singapore)\n\n# # Extract the summaries from each response object\n# ryanair_summaries = [item.summary for item in ryanair_summaries]\n# easyjet_summaries = [item.summary for item in easyjet_summaries]\n# qatarairways_summaries = [item.summary for item in qatarairways_summaries]\n# singaporeair_summaries = [item.summary for item in singaporeair_summaries]\n\n# # Create a list of company names for each airline\n# company_names = [['ryanair' for _ in ryanair_summaries], ['easyjet' for _ in easyjet_summaries], ['qatar' for _ in qatarairways_summaries], ['singapore' for _ in singaporeair_summaries]]\n\n# # Flatten the company names list\n# company_names = [name for sublist in company_names for name in sublist]\n\n# # Combine the summaries and company names into a dictionary\n# data_dict = {\n#     'company': company_names,\n#     'summary': ryanair_summaries + easyjet_summaries + qatarairways_summaries + singaporeair_summaries\n# }\n\n# # Create a dataframe from the dictionary and save it to a CSV file\n# df_summaries = pd.DataFrame(data_dict)\n# df_summaries.to_csv('/kaggle/working/df_summaries.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.131809Z","iopub.execute_input":"2024-03-01T10:00:33.132199Z","iopub.status.idle":"2024-03-01T10:00:33.144866Z","shell.execute_reply.started":"2024-03-01T10:00:33.13216Z","shell.execute_reply":"2024-03-01T10:00:33.143648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function that will accept the summaries of representative documents and create a single introduction summary for the company\n\ndef generate_summary_prompt(df, company_name):\n    # Get the summaries of representative documents from the dataframe\n    topic_representation = df['summary'].tolist()\n    \n    # Initialize an empty string to hold the text of the prompt\n    text_holder = ''\n    \n    # Iterate through the summaries, adding each one to the text holder\n    for index, topic in enumerate(topic_representation): \n        prompt_middle = f\"\"\"\\nDocument[{index}] : {topic_representation[index]}\\n\"\"\"\n        text_holder += prompt_middle\n\n    # Create the beginning and end of the prompt\n    prompt_bedining = f\"\"\"\nBelow [DOCS] delimited by /// are {index - 1} Documents, each document is labeled as \"Document[Number]\".\nEach Document is a user review of {company_name} the airline. Based on the Documents, write a Summary of {company_name}.\nThe Summary should be no more than 5 Sentences long.\n\\n[DOCS] /// \"\"\"\n\n    prompt_end = f\"\"\" ///\nThe tag line should not exceed 10 words. Each Tag line should be similar in Tone and format. \n\"\"\"\n\n    # Combine the beginning, middle, and end of the prompt\n    prompt = prompt_bedining + text_holder + prompt_end\n\n    return prompt","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.146578Z","iopub.execute_input":"2024-03-01T10:00:33.147063Z","iopub.status.idle":"2024-03-01T10:00:33.160235Z","shell.execute_reply.started":"2024-03-01T10:00:33.147018Z","shell.execute_reply":"2024-03-01T10:00:33.159267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Generate summaries for each company based on the representative document summaries\n# # NB Uncomment to Run, Update API Key & Beware of API Limits!\n\n# # Read in the dataframe containing all summaries\n# df_all_summaries = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_summaries.csv')\n\n# # Filter the dataframe for each company\n# sum_ryan = df_all_summaries[df_all_summaries['company'] == 'ryanair']\n# sum_easy = df_all_summaries[df_all_summaries['company'] == 'easyjet']\n# sum_sing = df_all_summaries[df_all_summaries['company'] == 'singapore']\n# sum_qatar = df_all_summaries[df_all_summaries['company'] == 'qatar']\n\n# # Create a dictionary to hold the dataframes for each company\n# run_cal = {'ryanair':sum_ryan, \"easyjet\": sum_easy, 'singapore':sum_sing, 'qatar': sum_qatar}\n\n# # Define the API key for Cohere\n# my_api_key = [YOUR_API_KEY]\n\n# # Initialize a Cohere client\n# co = cohere.Client(my_api_key)\n\n# # Initialize a dictionary to hold the summaries for each company\n# summary_holder = {}\n\n# # Loop through each company and generate a summary\n# for key, value in run_cal.items():\n#     # Initialize a Cohere client\n#     co = cohere.Client(my_api_key)\n    \n#     # Generate a summary prompt for the company\n#     prompt = generate_summary_prompt(value, key)\n\n#     # Generate a summary using Cohere\n#     response_summary = co.generate(  \n#     model='command-light',  \n#     prompt = prompt,  \n#     max_tokens=200,\n#     temperature=0.9, \n#     truncate='END')\n    \n#     # Add the summary to the summary holder dictionary\n#     summary_holder[key] = response_summary.generations[0].text\n\n# # Create a dataframe from the summary holder\n# data = {\n#     'company' : ['ryanair', 'easyjet', 'singapore', 'qatar'], \n#     'company_summary' : [summary_holder['ryanair'], summary_holder['easyjet'], summary_holder['singapore'], summary_holder['qatar']]\n# }\n# df_findings = pd.DataFrame(data)\n\n# # Save the dataframe to a csv file\n# df_findings.to_csv('/kaggle/working/df_findings.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.162354Z","iopub.execute_input":"2024-03-01T10:00:33.162853Z","iopub.status.idle":"2024-03-01T10:00:33.178041Z","shell.execute_reply.started":"2024-03-01T10:00:33.162817Z","shell.execute_reply":"2024-03-01T10:00:33.177113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a function to create a stacked bar chart summarizing representations per month\n\ndef plot_stacked_bar_with_line(df, company):\n    # Convert the 'updated_dates' column to datetime\n    df['updated_dates'] = pd.to_datetime(df['updated_dates'])\n    \n    # Extract the month and year from the 'updated_dates' column\n    df['Month_Year'] = df['updated_dates'].dt.to_period('M')\n\n    # Get the top 10 values in the 'Name' column\n    top_10_values = df['Name'].value_counts().head(11).index.tolist()\n\n    # Function to check if an item is in the top 10 values\n    def check_item(value):\n        if value in top_10_values:\n            return value.split(\"_\")[1]\n        else:\n            return \"other\"\n\n    # Apply the check_item function to create a new column 'Top10'\n    df['Top10'] = df['Name'].apply(lambda x: check_item(x))\n\n    # Group the data by 'Month_Year' and 'Top10' and get the count of each group\n    monthly_counts = df.groupby(['Month_Year', 'Top10']).size().reset_index(name='Count')\n    \n    # Group the data by 'Month_Year' and get the top 10 values for each month\n    top_10_per_month_year = monthly_counts.groupby('Month_Year').apply(lambda x: x.nlargest(11, 'Count')).reset_index(drop=True)\n    \n    # Pivot the table to get the 'Top10' values as columns\n    pivot_table = top_10_per_month_year.pivot(index='Month_Year', columns='Top10', values='Count').fillna(0)\n    \n    # Get the line data for the average star number per month\n    line_data = df.groupby('Month_Year')['star'].mean()\n    \n    # Create the figure and axis\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot the stacked bar chart\n    pivot_table.plot(kind='bar', stacked=True, ax=ax, color=color_pal)\n    \n    # Create a second y-axis for the line plot\n    ax2 = ax.twinx()\n    \n    # Plot the line chart for the average star number\n    ax2.plot(line_data.index.astype(str), line_data.values, color='red', marker='o', linestyle='-', linewidth=2, label='Average Star Number')\n    \n    # Set the ylabel for the second y-axis\n    ax2.set_ylabel('Average Star Number')\n    \n    # Set the legend for the first y-axis\n    ax.legend(loc='upper left', bbox_to_anchor=(1, 1), bbox_transform=fig.transFigure, title='\\n Feedback Category \\n')\n    \n    # Set the legend for the second y-axis\n    ax2.legend(loc='upper right')\n    \n    # Set the title and labels for the axes\n    plt.title(f'{company.capitalize()} Monthly User Feedback', fontdict=titles_dict)\n    ax.set_xlabel('Month_Year')\n    ax.set_ylabel('Count')\n\n    # Show the plot\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.17945Z","iopub.execute_input":"2024-03-01T10:00:33.179913Z","iopub.status.idle":"2024-03-01T10:00:33.197803Z","shell.execute_reply.started":"2024-03-01T10:00:33.179866Z","shell.execute_reply":"2024-03-01T10:00:33.196823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AIM : Read in DataFrames of Results of Clustering per Airlines\n\n# Read in the DataFrames\ndf_summaries = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_findings.csv')\ndf_representations = pd.read_csv('/kaggle/input/topic-modelling-outputs/df_summaries.csv')\ndf_clusters = pd.read_csv('/kaggle/input/topic-modelling-outputs/clustering_findings.csv')\n\n# Get the top line summary\ntop_line_summary = df_summaries['company_summary'].tolist()\n\n# Import Markdown to display the summary\nfrom IPython.display import Markdown","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.198861Z","iopub.execute_input":"2024-03-01T10:00:33.199231Z","iopub.status.idle":"2024-03-01T10:00:33.227924Z","shell.execute_reply.started":"2024-03-01T10:00:33.199199Z","shell.execute_reply":"2024-03-01T10:00:33.226695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_representations.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.229394Z","iopub.execute_input":"2024-03-01T10:00:33.229795Z","iopub.status.idle":"2024-03-01T10:00:33.241459Z","shell.execute_reply.started":"2024-03-01T10:00:33.22976Z","shell.execute_reply":"2024-03-01T10:00:33.240024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# AIM GENERATE A SUMMARY OF FINDINGS \n\ndef generate_summary_report_page2(df, company):\n    df_representations_ = df_representations[df_representations['company'] == company]\n    comment_summary_ = df_representations_['summary'].tolist()[1:]\n    \n    df_ = df.sort_values(by='Topic', ascending=True)\n    reps_ = df_['Representation'].unique()\n    reps_ = [item.split(\"'\")[1] for item in reps_][1:]\n    reps_comments_holder = f\"\"\" **Summary of {company}'s TrustPilot Feedback from Their Customers** \\n\n    Below you will find a summary explanation of the Feedback that {company} has recieved from their customers.\n    This feedback has been clustered together using BERTopic Modeling and the Text of the most representative Comments per Topic have been\n    Summarised using Cohere. Happy Reading! \\n\\n\n    \"\"\"\n\n    for index, item in enumerate(reps_):\n        paragraph_ = f\"\\n **{index+1}). {item}.**\\n\\n {comment_summary_[index]} \\n\"\n        reps_comments_holder += paragraph_\n    \n    return reps_comments_holder","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.243174Z","iopub.execute_input":"2024-03-01T10:00:33.243641Z","iopub.status.idle":"2024-03-01T10:00:33.252471Z","shell.execute_reply.started":"2024-03-01T10:00:33.243595Z","shell.execute_reply":"2024-03-01T10:00:33.2513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display Stacked Graph of Monthly User Custers\nplot_stacked_bar_with_line(df_ryan, 'EasyJet')","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.254048Z","iopub.execute_input":"2024-03-01T10:00:33.254605Z","iopub.status.idle":"2024-03-01T10:00:33.942733Z","shell.execute_reply.started":"2024-03-01T10:00:33.254561Z","shell.execute_reply":"2024-03-01T10:00:33.941524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the top line summary \nMarkdown(top_line_summary[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.944387Z","iopub.execute_input":"2024-03-01T10:00:33.944809Z","iopub.status.idle":"2024-03-01T10:00:33.952564Z","shell.execute_reply.started":"2024-03-01T10:00:33.94477Z","shell.execute_reply":"2024-03-01T10:00:33.951522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display Detailed Summary per Topic\nMarkdown(generate_summary_report_page2(df_easy, 'easyjet'))","metadata":{"execution":{"iopub.status.busy":"2024-03-01T10:00:33.95391Z","iopub.execute_input":"2024-03-01T10:00:33.954287Z","iopub.status.idle":"2024-03-01T10:00:33.970323Z","shell.execute_reply.started":"2024-03-01T10:00:33.954255Z","shell.execute_reply":"2024-03-01T10:00:33.968228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Project Conclusion üéâüìä <br>\n### Using BERTopic Topic Modelling, we've transformed a CSV of User-Generated Comments into actionable insights, including monthly topic breakdowns with sentiment scores, a summary of key findings and recommendations, and detailed explanations for each topic cluster. This approach can be adapted for any user-generated content. <br>\n\n**Project Journey:**\n- **Date Clean Up & Preprocessing:** We cleaned up DateTime Data & Saved EMbeddings to Save on Compute Time. \n- **Model Testing on Generic Comments:** By creating a generic comment column and experimenting with five pipeline approaches, we optimized our model before fine-tuning post-training. This included limiting the number of topics and providing custom labels for better performance. \n- **Fine Tuning & Model Training on Airline Specific Data:** To address corpus size differences, we adjusted the minimum number of clusters and used five separate documents (one for each airline) to train our model. We utilized the Cohere API and generative AI to create optimal topic labels. Our models were then saved both locally and on the Hugging Faces Hub. \n- **Leveraging Cohere LLM API & Generative AI:** In the final step, we leveraged Cohere's Generate Function to supply our topics and representative documents to the LLM, which then generated a summary and suggested actions, as well as a summary for each topic-representative document pair. Additionally, we created a table visualizing our airlines' monthly topic share alongside the sentiment score. ","metadata":{}}]}
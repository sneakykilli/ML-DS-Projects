{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7953459,"sourceType":"datasetVersion","datasetId":4665511}],"dockerImageVersionId":30675,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-27T07:17:41.433360Z","iopub.execute_input":"2024-03-27T07:17:41.433840Z","iopub.status.idle":"2024-03-27T07:17:41.458035Z","shell.execute_reply.started":"2024-03-27T07:17:41.433805Z","shell.execute_reply":"2024-03-27T07:17:41.456724Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install pip install sentence-transformers hdbscan bertopic cohere\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport hdbscan\nfrom hdbscan import HDBSCAN\nfrom sklearn.cluster import KMeans\nfrom bertopic import BERTopic\n\nfrom bertopic.representation import KeyBERTInspired\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom bertopic.representation import KeyBERTInspired\n\nimport cohere\nfrom bertopic.representation import Cohere\nfrom bertopic import BERTopic\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T05:43:14.578237Z","iopub.execute_input":"2024-03-27T05:43:14.578753Z","iopub.status.idle":"2024-03-27T05:46:04.965571Z","shell.execute_reply.started":"2024-03-27T05:43:14.578719Z","shell.execute_reply":"2024-03-27T05:46:04.964191Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add specific Branding & Styling to be used with plots & Bars\ncolor_pal = sns.color_palette(\"tab20c\")\nplt.style.use('fivethirtyeight')\n\ncolor_brand = ['#020B26', '#061440', '#0D2F73', '#1BBF91', '#F2F2F2']\n\npie_pal = sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True)\n\nnum_colors = 22\npie_pal = [pie_pal(i / num_colors) for i in range(num_colors)]\n\ntitles_dict = {'fontsize': 28,\n 'fontweight': 25,\n 'color':   color_brand[3]}\n\nsub_title_dict = {'fontsize': 20,\n 'fontweight': 18,\n 'color':   color_brand[3]}\n\nfig_text_dict = {\n    'color':   color_brand[3], \n}\n\ntextprops={'color': color_brand[2], 'fontsize':8}","metadata":{"execution":{"iopub.status.busy":"2024-03-27T07:18:23.935538Z","iopub.execute_input":"2024-03-27T07:18:23.936733Z","iopub.status.idle":"2024-03-27T07:18:23.951425Z","shell.execute_reply.started":"2024-03-27T07:18:23.936690Z","shell.execute_reply":"2024-03-27T07:18:23.950381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Aim: Discovering Topics and Themes within the Dataset\n\n### Action Plan\nUtilizing a subset of the dataset containing higher quality tweets, I'll preprocess the data and feed it into BERTopic, a pre-trained language model framework specifically designed for Topic Modeling.\n\n* **Step 1: Creating Vector Embeddings for a Subset of Twitter Data**  \n    - Generate vector embeddings for a subset of the Twitter dataset.\n\n* **Step 2: Utilizing BERTopic for Topic Generation & Cohere for Representative Topic Titles**  \n    - Feed the vector embeddings into BERTopic and utilize Cohere to generate representative topic titles.\n\n* **Step 3: Visualizing the Topics and Themes Returned by BERTopic, Merging Similar Ones**  \n    - Visualize all the topics and themes returned by BERTopic and merge the most similar ones.\n\n* **Step 4: Visualizing the Results in a Bar Chart**  \n    - Present the results in a bar chart for easier interpretation.","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Loading Pre-computed Embeddings for Topic Modeling\n\nThis step focuses on loading pre-computed tweet embeddings for use in the topic modeling process.\n\n1. **High-Quality Tweet Subset:**\n    * We start by reading a CSV file containing a high-quality subset of tweets (`df_higher_quality.csv`).\n    * A sample of 10,000 tweets (`df_subset`) is then extracted from this high-quality dataset for efficiency.\n\n2. **Pre-computed Embeddings:**\n    * The script attempts to load a file named `tweets_clean_high_quality.npy` from the `/kaggle/input/edgetier-takehome` directory. This file is assumed to contain pre-computed tweet embeddings generated earlier (potentially using `SentenceTransformer`).\n    * The `allow_pickle=True` argument is included as the embeddings might be a NumPy array containing pickled objects.\n\nBy utilizing pre-computed embeddings, we can avoid the potentially time-consuming process of re-encoding the tweets during topic modeling, improving efficiency. ","metadata":{}},{"cell_type":"code","source":"df_higher_quality = pd.read_csv('/kaggle/input/edgetier-takehome/df_higher_quality.csv')\ndf_subset = df_higher_quality[:10000] ","metadata":{"execution":{"iopub.status.busy":"2024-03-27T06:03:09.049137Z","iopub.execute_input":"2024-03-27T06:03:09.049679Z","iopub.status.idle":"2024-03-27T06:03:12.047138Z","shell.execute_reply.started":"2024-03-27T06:03:09.049638Z","shell.execute_reply":"2024-03-27T06:03:12.045954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sentence_transformers import SentenceTransformer\n\n# sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n# embeddings = sentence_model.encode(df_subset['tweet_cleaned'], show_progress_bar=True)\n# np.save('/kaggle/working/tweets_clean_high_quality.npy', embeddings)\n\nembeddings = np.load('/kaggle/input/edgetier-takehome/tweets_clean_high_quality.npy', allow_pickle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T06:03:29.485076Z","iopub.execute_input":"2024-03-27T06:03:29.485512Z","iopub.status.idle":"2024-03-27T06:05:27.735983Z","shell.execute_reply.started":"2024-03-27T06:03:29.485479Z","shell.execute_reply":"2024-03-27T06:05:27.734587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2: Building a Tweet Topic Model with BERTopic\n\nThis section utilizes BERTopic to create a topic model for our pre-processed tweets, revealing recurring themes within the data.\n\n1. **Pre-trained Embeddings & Vectorizer:**\n    * We load a pre-trained sentence embedding model for efficient text representation.\n    * A CountVectorizer tokenizes tweets, filters stop words, and considers bigrams.\n\n2. **Keyword Extraction & HDBSCAN:**\n    * A KeyBERTInspired model is used for keyword extraction (topic coherence).\n    * HDBSCAN clustering groups similar tweets with specific parameters.\n\n3. **Custom BERTopic Model:**\n    * We define a BERTopic model combining these components:\n        * Keyword extraction model (KeyBERTInspired)\n        * Custom HDBSCAN model\n        * CountVectorizer for tokenization\n        * Pre-trained sentence embedding model\n\n4. **Fitting & Generating Topics:**\n    * The model is fit on pre-processed tweets and embeddings.\n    * It outputs topic labels (`topics`) and cluster membership scores (`probs`) for each tweet.\n\nBERTopic helps us identify and categorize the main subjects discussed within the tweet data.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"my_cohere_api\"\nsecret_value = UserSecretsClient().get_secret(secret_label)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"co = cohere.Client(secret_value)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T06:43:38.334128Z","iopub.execute_input":"2024-03-27T06:43:38.334606Z","iopub.status.idle":"2024-03-27T06:43:38.355067Z","shell.execute_reply.started":"2024-03-27T06:43:38.334570Z","shell.execute_reply":"2024-03-27T06:43:38.353921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pre-trained sentence embedding model (Sentence Transformers library)\nembedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n# Define the vectorizer for tokenization (CountVectorizer from scikit-learn)\nvectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n\n# Define the keyword extraction model (KeyBERTInspired from BERTopic)\nrepresentation_model_cohere = Cohere(co, delay_in_seconds=12, model='command')\n# representation_model = KeyBERTInspired()\n\n# Access the pre-processed tweets from the dataframe\ndoc = df_subset['tweet_cleaned']\n\n# Define the HDBSCAN clustering model parameters\nhdbscan_model = HDBSCAN(\n    min_cluster_size=25,  # Minimum size of a cluster\n    metric='euclidean',     # Distance metric for clustering\n    cluster_selection_method='eom',  # Selection method for optimal cluster number ('eom' - Excess Of Mean)\n    prediction_data=True,    # Include cluster membership scores in output\n)\n\n# Define the BERTopic model with custom components\ntopic_model = BERTopic(\n    representation_model=representation_model_cohere,  # Keyword extraction model for topic coherence\n    hdbscan_model=hdbscan_model,  # HDBSCAN model for clustering\n    vectorizer_model=vectorizer_model,  # CountVectorizer for tokenization\n    embedding_model=embedding_model,  # Pre-trained sentence embedding model\n)\n\n# Fit the BERTopic model on the pre-processed tweets and generate topics\ntopics, probs = topic_model.fit_transform(doc, embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T06:43:50.892248Z","iopub.execute_input":"2024-03-27T06:43:50.893417Z","iopub.status.idle":"2024-03-27T06:52:33.719083Z","shell.execute_reply.started":"2024-03-27T06:43:50.893365Z","shell.execute_reply":"2024-03-27T06:52:33.717746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Exploring Tweet Topics with Visualization\n\nHere, we analyze and visualize the extracted topics:\n\n1. **Merging Related Topics:** Based on domain knowledge or analysis, topics are merged (e.g., topics 25, 3, and 1) using `topic_model.merge_topics`.\n\n2. **Topic Hierarchy View:** The `visualize_hierarchy` method creates a visual representation of topic relationships.\n\n3. **Topic Details & Visualization:** \n    * Topic information (names, counts) is retrieved using `get_topic_info`.\n    * A bar chart visualizes the topic distribution with percentages overlaid.","metadata":{}},{"cell_type":"code","source":"topic_model.visualize_topics()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T06:53:50.397106Z","iopub.execute_input":"2024-03-27T06:53:50.398113Z","iopub.status.idle":"2024-03-27T06:53:51.692648Z","shell.execute_reply.started":"2024-03-27T06:53:50.398068Z","shell.execute_reply":"2024-03-27T06:53:51.691378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_hierarchy(title='Clustering')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T06:53:54.543547Z","iopub.execute_input":"2024-03-27T06:53:54.543968Z","iopub.status.idle":"2024-03-27T06:53:54.634155Z","shell.execute_reply.started":"2024-03-27T06:53:54.543937Z","shell.execute_reply":"2024-03-27T06:53:54.632566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topics_to_merge = [\n[25, 3, 1, 0, 10, 2],\n[14, 12, 16, 11],\n[20, 21, 4, 29, 6, 19],\n[34, 32, 31, 17, 18, 24, 5, 9, 36],\n[23, 33, 27],\n[28, 22, 15, 7, 8, 13, 30],\n[35, 26],\n]\ntopic_model.merge_topics(doc, topics_to_merge)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T07:13:46.130844Z","iopub.execute_input":"2024-03-27T07:13:46.131548Z","iopub.status.idle":"2024-03-27T07:15:52.389526Z","shell.execute_reply.started":"2024-03-27T07:13:46.131493Z","shell.execute_reply":"2024-03-27T07:15:52.388200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topic_model.visualize_hierarchy(title='Clustering')","metadata":{"execution":{"iopub.status.busy":"2024-03-27T07:16:02.984213Z","iopub.execute_input":"2024-03-27T07:16:02.984912Z","iopub.status.idle":"2024-03-27T07:16:03.055883Z","shell.execute_reply.started":"2024-03-27T07:16:02.984870Z","shell.execute_reply":"2024-03-27T07:16:03.054573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_df = topic_model.get_topic_info()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T07:16:12.149084Z","iopub.execute_input":"2024-03-27T07:16:12.149610Z","iopub.status.idle":"2024-03-27T07:16:12.178050Z","shell.execute_reply.started":"2024-03-27T07:16:12.149571Z","shell.execute_reply":"2024-03-27T07:16:12.176864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport textwrap\n\n# Extract topic names from the info dataframe\ntopic_names = info_df['Name']\n\n# Create a list of positions corresponding to topic names (y-axis labels)\npositions = range(len(topic_names))\n\n# Extract the total count of tweets assigned to each topic\ntotal_count = info_df['Count']\n\n# Calculate the percentage of tweets for each topic (rounded to 2 decimal places)\ncount_percent = [(round(item / 10000, 2) * 100) for item in info_df['Count']]\n\n# Define the bar height for the chart\nbar_height = 0.90\n\n# Create a figure and primary axis for plotting bar chart\nfig, ax1 = plt.subplots(figsize=(12, 8))\n\n# Create horizontal bar chart to represent total tweet count for each topic\nax1.barh(positions, total_count, height=bar_height, label=f\"Tweet Topics\", color=color_brand[2])\n\n# Set labels and title for the primary axis (x-axis and y-axis)\nax1.set_ylabel('Topics in Tweets')\nax1.set_xlabel('Total Number')\nax1.set_title('Topics in Tweets', fontdict=titles_dict)\n\n# Set y-axis tick positions and labels (topic names)\nax1.set_yticks(positions)\nwrapped_labels = [textwrap.fill(label, 15) for label in topic_names]\nax1.set_yticklabels(wrapped_labels)\n\nax1.set_yticklabels(topic_names)\n\n# Add legend for the chart\nax1.legend(loc='lower right')\n\n# Create a secondary axis that shares the y-axis with the primary axis\nax2 = ax1.twiny()\n\n# Plot a line representing the percentage of tweets for each topic (no line width)\nax2.plot(count_percent, positions, color=color_brand[2], linewidth=0)\n\n# Set label for the secondary axis (percentage)\nax2.set_xlabel('Percentage of Total')\n\n# Annotate each bar with the corresponding percentage value (with slight horizontal offset)\nfor i, value in enumerate(count_percent):\n    ax2.text(value + 0.1, i, f'{value}%', color=color_brand[3], ha='left', va='center', fontdict=sub_title_dict)\n\n# Adjust layout to prevent overlapping elements\nplt.tight_layout()\n\n# Display the chart\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T08:02:11.544361Z","iopub.execute_input":"2024-03-27T08:02:11.544855Z","iopub.status.idle":"2024-03-27T08:02:12.702377Z","shell.execute_reply.started":"2024-03-27T08:02:11.544822Z","shell.execute_reply":"2024-03-27T08:02:12.701213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport textwrap\n\n# Extract topic names from the info dataframe\ntopic_names = info_df['Name']\n\n# Create a list of positions corresponding to topic names (y-axis labels)\npositions = range(len(topic_names))\n\n# Extract the total count of tweets assigned to each topic\ntotal_count = info_df['Count']\n\n# Calculate the percentage of tweets for each topic (rounded to 2 decimal places)\ncount_percent = [(round(item / 10000, 2) * 100) for item in info_df['Count']]\n\n# Define the bar height for the chart\nbar_height = 0.90\n\n# Create a figure and primary axis for plotting bar chart\nfig, ax1 = plt.subplots(figsize=(12, 8))\n\n# Create horizontal bar chart to represent total tweet count for each topic\nbars = ax1.barh(positions, total_count, height=bar_height, label=f\"Tweet Topics\", color=color_brand[2])\n\n# Set labels and title for the primary axis (x-axis and y-axis)\nax1.set_ylabel('Topics in Tweets')\nax1.set_xlabel('Total Number')\nax1.set_title('Topics in Tweets', fontdict=titles_dict)\n\n# Set y-axis tick positions and labels (topic names)\nax1.set_yticks(positions)\n# Wrap the x-axis labels\nwrapped_labels = [textwrap.fill(label, 60) for label in topic_names]\nax1.set_yticklabels(wrapped_labels)\n\n# Add legend for the chart\nax1.legend(loc='lower right')\n\n# Create a secondary axis that shares the y-axis with the primary axis\nax2 = ax1.twiny()\n\n# Plot a line representing the percentage of tweets for each topic (no line width)\nax2.plot(count_percent, positions, color=color_brand[2], linewidth=0)\n\n# Set label for the secondary axis (percentage)\nax2.set_xlabel('Percentage of Total')\n\n# Annotate each bar with the corresponding percentage value (with slight horizontal offset)\nfor i, value in enumerate(count_percent):\n    ax2.text(value + 0.1, i, f'{value}%', color=color_brand[3], ha='left', va='center', fontdict=sub_title_dict)\n\n# Adjust layout to prevent overlapping elements\nplt.tight_layout()\n\n# Display the chart\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-27T08:04:15.991125Z","iopub.execute_input":"2024-03-27T08:04:15.991589Z","iopub.status.idle":"2024-03-27T08:04:17.139716Z","shell.execute_reply.started":"2024-03-27T08:04:15.991554Z","shell.execute_reply":"2024-03-27T08:04:17.138328Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
